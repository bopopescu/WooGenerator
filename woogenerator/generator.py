"""
Generator extracts data from a master and slave database of product information,
then synchronizes that information on several layers.
In a typical configuration, the master is a spreadsheet of product information
in the generator format, which is a custom heirarchical representation of product
and category data designed for minimal redundancy.
The generator format is a hybrid of a tree structure with a flat-file table.
Product titles and SKUs are generated by concatenating the name and code
fragments of a product's ancestors in a tree.

These are the steps that generator uses to synchronize products:
 - The master information is optionally downloaded from Google Drive if the
 `--download-master` flag is set, or it is read from local csv files.
 - Master information is parsed into a master `CSVParse` object which contains
 several `Import` objects which correspond to products, variations, images,
 categories and other objects within the database.
 - The master information is exported as a set of CSV file which is compatible with the
 WooCommerce Product CSV Import Suite plugin. At this point the program can be
 terminated if the `--master-and-quit` command line flag is set.
 - Slave product information is optionally downloaded from the the selected API
 and parsed in a similar way to master if the `--download-slave` flag is set,
 otherwise it is parsed from local json files
 - The objectes from these parser objects are matched with each other into a
 collection of `match` objects which need special attention when the `matcher`
 objects encounter duplicates. Matching has to be done separately for images,
 categories, products and product variations in this order since each of these
 layers are inter-dependent.
 - The `match` objects are analysed and a collection of `sync_update` objects
 are generated which describe how to update the databases so that they match.
 This process is referred to as merging.
 - Before any updates are done, some HTML reports are generated and the user
 is asked for confirmation before continuing.
 - The updates are then carried out on the slave database using a `sync_client`
 for each layer.

"""

from __future__ import absolute_import


# to run full sync test on TT
"""
python -m woogenerator.generator \
    --testmode --schema "TT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --download-master --download-slave \
    --do-categories --do-images --do-specials \
    --do-sync --update-slave --do-problematic --auto-create-new --ask-before-update \
    --wp-srv-offset -36000 \
    -vvv --debug-trace
"""

# To run full sync test on VT
"""
python -m woogenerator.generator \
    --testmode --schema "VT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --local-test-config 'generator_config_test_vt.yaml' \
    --download-master --download-slave \
    --do-categories --do-specials \
    --do-images --do-resize-images \
    --do-sync --update-slave --do-problematic --auto-create-new --ask-before-update \
    --wp-srv-offset -36000 \
    -vvv --debug-trace
"""

# TT just master and quit
"""
python -m woogenerator.generator \
    --testmode --schema "TT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --download-master --download-slave \
    --do-categories --do-images --do-variations --master-and-quit \
    --do-specials --specials-mode 'auto_next' \
    --do-sync --do-problematic --auto-create-new \
    --wp-srv-offset -36000 \
    -vvv --debug-trace
"""

## VT just master and quit
"""
python -m woogenerator.generator \
    --testmode --schema "VT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --local-test-config 'generator_config_test_vt.yaml' \
    --download-master --download-slave \
    --do-categories --do-images --do-variations --master-and-quit \
    --do-specials --specials-mode 'all_future' \
    --do-sync --do-problematic --auto-create-new \
    --wp-srv-offset -36000 \
    -vvv --debug-trace
"""

import io
import os
import shutil
import sys
import time
import traceback
import webbrowser
import zipfile
from bisect import insort
from collections import OrderedDict
from pprint import pformat, pprint

from exitstatus import ExitStatus
from requests.exceptions import ConnectionError, ConnectTimeout, ReadTimeout

from .images import process_images
from .matching import (AttacheeSkuMatcher, AttachmentIDMatcher,
                       CategoryMatcher, ImageMatcher, ProductMatcher,
                       VariationMatcher, AttacheeTitleMatcher)
from .namespace.core import (MatchNamespace, ParserNamespace, ResultsNamespace,
                             UpdateNamespace)
from .namespace.prod import SettingsNamespaceProd
from .parsing.api import ApiParseWoo
from .parsing.dyn import CsvParseDyn
from .parsing.special import CsvParseSpecial
from .parsing.woo import WooCatList
from .utils import (ProgressCounter, Registrar, SanitationUtils, SeqUtils,
                    TimeUtils)
from .utils.reporter import (ReporterNamespace, do_cat_sync_gruop,
                             do_category_matches_group, do_delta_group,
                             do_duplicates_group, do_duplicates_summary_group,
                             do_failures_group, do_img_sync_group,
                             do_main_summary_group, do_matches_group,
                             do_matches_summary_group, do_post_summary_group,
                             do_successes_group, do_sync_group,
                             do_variation_matches_group,
                             do_var_sync_group)


def timediff(settings):
    """Return time elapsed since start."""
    return time.time() - settings.start_time


def check_warnings(settings):
    """
    Check if there have been any errors or warnings registered in Registrar.

    Raise approprriate exceptions if needed
    """
    if Registrar.errors:
        print("there were some urgent errors "
              "that need to be reviewed before continuing")
        Registrar.print_message_dict(0)
        status = ExitStatus.failure
        print "\nexiting with status %s\n" % status
        if Registrar.DEBUG_TRACE:
            import pudb; pudb.set_trace()
        sys.exit(status)

    elif Registrar.warnings:
        print "there were some warnings that should be reviewed"
        Registrar.print_message_dict(1)


def populate_master_parsers(parsers, settings):
    """Create and populates the various parsers."""
    Registrar.register_message('schema: %s, woo_schemas: %s' % (
        settings.schema, settings.woo_schemas
    ))

    parsers.dyn = CsvParseDyn()
    parsers.special = CsvParseSpecial()

    if Registrar.DEBUG_GEN:
        Registrar.register_message(
            "master_download_client_args: %s" %
            settings.master_download_client_args)

    with settings.master_download_client_class(**settings.master_download_client_args) as client:

        if settings.schema_is_woo:
            if settings.do_dyns:
                Registrar.register_message("analysing dprc rules")
                client.analyse_remote(
                    parsers.dyn,
                    data_path=settings.dprc_path,
                    gid=settings.dprc_gid
                )
                settings.dprc_rules = parsers.dyn.taxos

                Registrar.register_message("analysing dprp rules")
                parsers.dyn.clear_transients()
                client.analyse_remote(
                    parsers.dyn,
                    data_path=settings.dprp_path,
                    gid=settings.dprp_gid
                )
                settings.dprp_rules = parsers.dyn.taxos

            if settings.do_specials:
                Registrar.register_message("analysing specials")
                client.analyse_remote(
                    parsers.special,
                    data_path=settings.specials_path,
                    gid=settings.spec_gid
                )
                if Registrar.DEBUG_SPECIAL:
                    Registrar.register_message(
                        "all specials: %s" % parsers.special.tabulate()
                    )

                settings.special_rules = parsers.special.rules

                settings.current_special_groups = parsers.special.determine_current_spec_grps(
                    specials_mode=settings.specials_mode,
                    current_special=settings.current_special
                )
                if Registrar.DEBUG_SPECIAL:
                    Registrar.register_message(
                        "current_special_groups: %s" % settings.current_special_groups
                    )

        master_parser_args = settings.master_parser_args

        if os.path.exists(settings.master_path):
            master_mod_ts = max(
                os.path.getmtime(settings.master_path), os.path.getctime(settings.master_path)
            )
            master_mod_dt = TimeUtils.timestamp2datetime(master_mod_ts)
            master_parser_args['defaults'].update({
                'modified_local': master_mod_dt,
                'modified_gmt': TimeUtils.datetime_local2gmt(master_mod_dt)
            })


        parsers.master = settings.master_parser_class(
            **master_parser_args
        )

        Registrar.register_progress("analysing master product data")

        analysis_kwargs = {
            'data_path': settings.master_path,
            'gid': settings.gen_gid,
            'limit': settings['master_parse_limit']
        }
        if Registrar.DEBUG_PARSER:
            Registrar.register_message("analysis_kwargs: %s" % analysis_kwargs)

        client.analyse_remote(parsers.master, **analysis_kwargs)

        if Registrar.DEBUG_PARSER and hasattr(
                parsers.master, 'categories_name'):
            for category_name, category_list in getattr(
                    parsers.master, 'categories_name').items():
                if len(category_list) < 2:
                    continue
                if SeqUtils.check_equal(
                        [category.namesum for category in category_list]):
                    continue
                Registrar.register_warning("bad category: %50s | %d | %s" % (
                    category_name[:50], len(category_list), str(category_list)
                ))

        return parsers


def populate_slave_parsers(parsers, settings):
    """Populate the parsers for data from the slave database."""

    parsers.slave = settings.slave_parser_class(**settings.slave_parser_args)

    slave_client_class = settings.slave_download_client_class
    slave_client_args = settings.slave_download_client_args

    # with ProdSyncClientWC(settings['slave_wp_api_params']) as client:

    if settings.schema_is_woo and settings.do_categories:
        Registrar.register_progress("analysing API category data")

        cat_sync_client_class = settings.slave_cat_sync_client_class
        cat_sync_client_args = settings.slave_cat_sync_client_args

        with cat_sync_client_class(**cat_sync_client_args) as client:
            client.analyse_remote_categories(
                parsers.slave,
                data_path=settings.slave_cat_path
            )

    # TODO: ignore products which are post_status = trash

    with slave_client_class(**slave_client_args) as client:

        Registrar.register_progress("analysing API product data")

        client.analyse_remote(
            parsers.slave,
            data_path=settings.slave_path
        )

    if settings.schema_is_woo and settings.do_images:
        Registrar.register_progress("analysing API image data")
        img_client_class = settings.slave_img_sync_client_class
        img_client_args = settings.slave_img_sync_client_args

        with img_client_class(**img_client_args) as client:
            client.analyse_remote_imgs(
                parsers.slave,
                data_path=settings.slave_img_path,
                skip_unattached_images=settings.skip_unattached_images
            )

    if Registrar.DEBUG_CLIENT:
        container = settings.slave_parser_class.product_container.container
        prod_list = container(parsers.slave.products.values())
        Registrar.register_message("Products: \n%s" % prod_list.tabulate())

    return parsers

def export_master_parser(settings, parsers):
    """Export key information from master parser to csv."""
    Registrar.register_progress("Exporting Master info to disk")

    product_colnames = settings.coldata_class.get_col_values_native('path', target='wc-csv')

    for col in settings.exclude_cols:
        if col in product_colnames:
            del product_colnames[col]

    if settings.schema_is_woo and settings.do_attributes:
        attribute_colnames = settings.coldata_class.get_attribute_colnames_native(
            parsers.master.attributes, parsers.master.vattributes)
        product_colnames = SeqUtils.combine_ordered_dicts(
            product_colnames, attribute_colnames
        )

    container = parsers.master.product_container.container

    product_list = container(parsers.master.products.values())
    product_list.export_items(settings.fla_path, product_colnames)

    if settings.schema_is_woo:
        # variations
        variation_container = settings.master_parser_class.variation_container.container
        # variation_cols = settings.coldata_class_var.get_col_data_native('write', target='wc-csv')
        variation_col_names = settings.coldata_class_var.get_col_values_native('path', target='wc-csv')
        attribute_meta_col_names = settings.coldata_class_var.get_attribute_meta_colnames_native(
            parsers.master.vattributes)
        variation_col_names = SeqUtils.combine_ordered_dicts(
            variation_col_names, attribute_meta_col_names
        )
        if settings.do_variations and parsers.master.variations:

            variation_list = variation_container(parsers.master.variations.values())
            variation_list.export_items(settings.flv_path, variation_col_names)

            updated_variations = parsers.master.updated_variations.values()

            if updated_variations:
                updated_variations_list = variation_container(updated_variations)
                updated_variations_list.export_items(
                    settings.flvu_path, variation_col_names
                )

        # categories
        if settings.do_categories and parsers.master.categories:
            # category_cols = settings.coldata_class_cat.get_col_data_native('write', target='wc-csv')
            category_col_names = settings.coldata_class_cat.get_col_values_native('path', target='wc-csv')
            for col in settings.exclude_cols_cat:
                if col in category_col_names:
                    del category_col_names[col]
            category_container = settings.master_parser_class.category_container.container
            category_list = category_container([
                category for category in parsers.master.categories.values()
                if category.members
            ])
            category_list.export_items(settings.cat_path, category_col_names)

        # specials
        if settings.do_specials and settings.current_special_id:
            special_products = parsers.master.onspecial_products.values()
            if special_products:
                special_product_list = container(special_products)
                special_product_list.export_items(
                    settings.fls_path, product_colnames
                )
            special_variations = parsers.master.onspecial_variations.values()
            if special_variations:
                sp_variation_list = variation_container(special_variations)
                sp_variation_list.export_items(
                    settings.flvs_path, variation_col_names
                )

        updated_products = parsers.master.updated_products.values()
        if updated_products:
            updated_product_list = container(updated_products)
            updated_product_list.export_items(
                settings.flu_path, product_colnames
            )

    Registrar.register_progress("CSV Files have been created.")

def cache_api_data(settings, parsers):
    """Export key information from slave parser to csv."""
    if not settings.download_slave:
        return

    Registrar.register_progress("Exporting Slave info to disk")
    container = settings.slave_parser_class.product_container.container
    product_list = container(parsers.slave.products.values())
    product_list.export_api_data(settings.slave_path)

    if settings.do_categories and parsers.slave.categories:
        category_container = settings.slave_parser_class.category_container.container
        category_list = category_container(parsers.slave.categories.values())
        category_list.export_api_data(settings.slave_cat_path)

    if settings.do_images and parsers.slave.attachments:
        attachment_container = settings.slave_parser_class.attachment_container.container
        image_list = attachment_container(parsers.slave.attachments.values())
        image_list.export_api_data(settings.slave_img_path)

def do_match_images(parsers, matches, settings):
    if Registrar.DEBUG_IMG:
        Registrar.register_message(
            "matching %d master attachments with %d slave attachments" %
            (len(parsers.master.attachments),
             len(parsers.slave.attachments)))

    matches.image = MatchNamespace(
        index_fn=ImageMatcher.image_index_fn
    )

    if Registrar.DEBUG_TRACE:
        import pudb; pudb.set_trace()

    image_matcher = ImageMatcher()
    image_matcher.clear()
    slave_imgs_attachments = OrderedDict([
        (index, image) for index, image in parsers.slave.attachments.items()
        if image.attaches.has_products_categories
    ])
    master_imgs_attachments = OrderedDict([
        (index, image) for index, image in parsers.master.attachments.items()
        if image.attaches.has_products_categories
    ])
    image_matcher.process_registers(
        slave_imgs_attachments, master_imgs_attachments
    )

    matches.image.globals.add_matches(image_matcher.pure_matches)
    matches.image.masterless.add_matches(image_matcher.masterless_matches)
    matches.image.slaveless.add_matches(image_matcher.slaveless_matches)

    if Registrar.DEBUG_IMG:
        if image_matcher.pure_matches:
            Registrar.register_message("All Image matches:\n%s" % (
                '\n'.join(map(str, image_matcher.matches))))

    matches.image.valid += image_matcher.pure_matches

    if not image_matcher.duplicate_matches:
        return matches

    extra_valid_indices_m = set()
    extra_valid_indices_s = set()

    matches.image.duplicate['file_name'] = image_matcher.duplicate_matches

    filename_duplicate_indices_m = set([
        attachment.index \
        for match in image_matcher.duplicate_matches
        for attachment in match.m_objects
    ])
    filename_duplicate_indices_s = set([
        attachment.index \
        for match in image_matcher.duplicate_matches
        for attachment in match.s_objects
    ])

    for match in image_matcher.duplicate_matches:
        if Registrar.DEBUG_IMG or Registrar.DEBUG_TRACE:
            Registrar.register_message(
                "analysing duplicate match:\n%s" % match.tabulate()
            )
        attachee_sku_sub_matches = image_matcher.find_attachee_sku_matches(match)
        for key, match in attachee_sku_sub_matches.items():
            if Registrar.DEBUG_IMG or Registrar.DEBUG_TRACE:
                Registrar.register_message(
                    "sub match %s is %s:\n%s" % (
                        key,
                        match.type,
                        match.tabulate()
                    )
                )
            if match.type in ['pure', 'masterless', 'slaveless'] \
            or match.type == 'duplicate' and match.m_len == 1:
                extra_valid_indices_m.update([
                    attachment.index for attachment in match.m_objects
                ])
                extra_valid_indices_s.update([
                    attachment.index for attachment in match.s_objects
                ])
                if match.type in ['pure', 'duplicate']:
                    matches.image.valid += [match]
                elif match.type == 'masterless':
                    matches.image.masterless.add_matches([match])
                elif match.type == 'slaveless':
                    matches.image.slaveless.add_matches([match])
            else:
                exc = UserWarning(
                    (
                        "Could not match image, most likely because multiple "
                        "images with the same name are attached to the same SKU.\n%s"
                    ) % (
                        match.tabulate()
                    )
                )
                Registrar.register_warning(exc)

    try:
        assert \
        extra_valid_indices_m.issuperset(filename_duplicate_indices_m), \
        (
            "all master indices from filename duplicates should be contained in "
            "extra attachee match indices:\nfilename:\n%s\nattachee_indices:\n%s"
        ) % (
            filename_duplicate_indices_m,
            extra_valid_indices_m
        )
        assert \
        extra_valid_indices_s.issuperset(filename_duplicate_indices_s), \
        (
            "all slave indices from filename duplicates should be contained in "
            "extra attachee match indices:\nfilename:\n%s\nattachee_indices:\n%s"
        ) % (
            filename_duplicate_indices_s,
            extra_valid_indices_s
        )
    except AssertionError as exc:
        warn = RuntimeWarning(
            "could not match all images.\n%s\n%s" % (
                "\n".join([
                    "%s:\n%s" % (key, dup_matches.tabulate()) \
                    for key, dup_matches in matches.image.duplicate.items()
                ]),
                str(exc)
            )
        )
        Registrar.register_warning(warn)
        if Registrar.DEBUG_TRACE:
            import pudb; pudb.set_trace()

    if Registrar.DEBUG_IMG or Registrar.DEBUG_TRACE:
        Registrar.register_message("all matches:\n%s" % matches.image.tabulate())

    return matches

def do_match_categories(parsers, matches, settings):

    if Registrar.DEBUG_CATS:
        Registrar.register_message(
            "matching %d master categories with %d slave categories" %
            (len(parsers.master.categories),
             len(parsers.slave.categories)))

    matches.category = MatchNamespace(
        index_fn=CategoryMatcher.category_index_fn
    )

    if not( parsers.master.categories and parsers.slave.categories ):
        return matches

    category_matcher = CategoryMatcher()
    category_matcher.clear()
    category_matcher.process_registers(
        parsers.slave.categories, parsers.master.categories
    )

    matches.category.globals.add_matches(category_matcher.pure_matches)
    matches.category.masterless.add_matches(
        category_matcher.masterless_matches)
    # matches.deny_anomalous(
    #     'category_matcher.masterless_matches', category_matcher.masterless_matches
    # )
    matches.category.slaveless.add_matches(category_matcher.slaveless_matches)
    # matches.deny_anomalous(
    #     'category_matcher.slaveless_matches', category_matcher.slaveless_matches
    # )

    if Registrar.DEBUG_CATS:
        if category_matcher.pure_matches:
            Registrar.register_message("All Category matches:\n%s" % (
                '\n'.join(map(str, category_matcher.matches))))

    # using valid because the category tree can collapse multiple master categories into single slave

    matches.category.valid += category_matcher.pure_matches

    if category_matcher.duplicate_matches:
        matches.category.duplicate['title'] = category_matcher.duplicate_matches

        for match in category_matcher.duplicate_matches:
            master_taxo_sums = [cat.namesum for cat in match.m_objects]
            if all(master_taxo_sums) \
                    and SeqUtils.check_equal(master_taxo_sums) \
                    and not len(match.s_objects) > 1:
                matches.category.valid.append(match)
            else:
                matches.category.invalid.append(match)
        if matches.category.invalid:
            exc = UserWarning(
                "categories couldn't be synchronized because of ambiguous names:\n%s"
                % '\n'.join(map(str, matches.category.invalid)))
            Registrar.register_error(exc)
            raise exc

    if category_matcher.slaveless_matches and category_matcher.masterless_matches:
        exc = UserWarning(
            "You may want to fix up the following categories before syncing:\n%s\n%s"
            %
            ('\n'.join(map(str, category_matcher.slaveless_matches)),
             '\n'.join(map(str, category_matcher.masterless_matches))))
        Registrar.register_error(exc)
        # raise exc

    return matches

# TODO: do_match_attributes ?
def do_match_attributes(parsers, matches, settings):
    if settings.do_attributes:
        raise NotImplementedError("Do Match Attributes not implemented")

def do_match_prod(parsers, matches, settings):
    """For every item in slave, find its counterpart in master."""

    Registrar.register_progress("Attempting matching")

    if not settings.do_sync:
        return matches

    product_matcher = ProductMatcher()
    product_matcher.process_registers(
        parsers.slave.products, parsers.master.products
    )
    # print product_matcher.__repr__()

    matches.globals.add_matches(product_matcher.pure_matches)
    matches.masterless.add_matches(product_matcher.masterless_matches)
    matches.deny_anomalous(
        'product_matcher.masterless_matches', product_matcher.masterless_matches
    )
    matches.slaveless.add_matches(product_matcher.slaveless_matches)
    matches.deny_anomalous(
        'product_matcher.slaveless_matches', product_matcher.slaveless_matches
    )

    try:
        matches.deny_anomalous(
            'product_matcher.duplicate_matches',
            product_matcher.duplicate_matches,
            True
        )
    except AssertionError as exc:
        exc = UserWarning(
            "products couldn't be synchronized because of ambiguous SKUs:%s"
            % '\n'.join(map(str, product_matcher.duplicate_matches)))
        Registrar.register_error(exc)
        raise exc

def do_match_var(parsers, matches, settings):
    matches.variation = MatchNamespace(
        index_fn=ProductMatcher.product_index_fn
    )

    if not settings['do_variations']:
        return

    variation_matcher = VariationMatcher()
    variation_matcher.process_registers(
        parsers.slave.variations, parsers.master.variations
    )

    if Registrar.DEBUG_VARS:
        Registrar.register_message("variation matcher:\n%s" %
                                   variation_matcher.__repr__())

    matches.variation.globals.add_matches(variation_matcher.pure_matches)
    matches.variation.masterless.add_matches(
        variation_matcher.masterless_matches)
    matches.variation.deny_anomalous(
        'variation_matcher.masterless_matches',
        variation_matcher.masterless_matches
    )
    matches.variation.slaveless.add_matches(
        variation_matcher.slaveless_matches)
    matches.variation.deny_anomalous(
        'variation_matcher.slaveless_matches',
        variation_matcher.slaveless_matches
    )
    if variation_matcher.duplicate_matches:
        matches.variation.duplicate['index'] = variation_matcher.duplicate_matches


def do_merge_images(matches, parsers, updates, settings):
    updates.image = UpdateNamespace()

    if not hasattr(matches, 'image'):
        return updates

    sync_handles = settings.sync_handles_img

    if Registrar.DEBUG_TRACE:
        import pudb; pudb.set_trace()

    for match in matches.image.valid:
        m_object = match.m_object
        for s_object in match.s_objects:

            sync_update = settings.syncupdate_class_img(m_object, s_object)

            sync_update.update(sync_handles)

            if not sync_update.important_static:
                updates.image.problematic.append(sync_update)
                continue

            if sync_update.m_updated:
                updates.image.master.append(sync_update)

            if sync_update.s_updated:
                updates.image.slave.append(sync_update)

    if settings['auto_create_new']:
        for count, match in enumerate(matches.image.slaveless):
            m_object = match.m_object
            Registrar.register_message(
                "will create image %d: %s" % (
                    count, m_object.identifier
                )
            )
            if not (m_object.attaches.products or m_object.attaches.categories):
                continue

            empty_s_object = parsers.slave.get_empty_attachment_instance()
            sync_update = settings.syncupdate_class_img(
                m_object, empty_s_object
            )
            sync_update.update(sync_handles)
            updates.image.new_slaves.append(sync_update)

    return updates

def do_merge_categories(matches, parsers, updates, settings):
    updates.category = UpdateNamespace()

    if not hasattr(matches, 'category'):
        return updates

    sync_handles = settings.sync_handles_cat

    # if Registrar.DEBUG_TRACE:
    #     import pudb; pudb.set_trace()

    for match in matches.category.valid:
        s_object = match.s_object
        for m_object in match.m_objects:
            # m_object = match.m_objects[0]

            sync_update = settings.syncupdate_class_cat(m_object, s_object)

            sync_update.update(sync_handles)

            if settings.do_images:
                sync_update.simplify_sync_warning_value_singular('image', ['id', 'title', 'source_url'])

            if not sync_update.important_static:
                updates.category.problematic(sync_update)
                continue

            if sync_update.m_updated:
                updates.category.master.append(sync_update)

            if sync_update.s_updated:
                updates.category.slave.append(sync_update)

    if settings['auto_create_new']:
        for count, match in enumerate(matches.category.slaveless):
            m_object = match.m_object
            Registrar.register_message(
                "will create category %d: %s" % (
                    count, m_object.identifier
                )
            )
            empty_s_object = parsers.slave.get_empty_category_instance()
            sync_update = settings.syncupdate_class_cat(m_object, empty_s_object)
            sync_update.update(sync_handles)
            updates.category.new_slaves.append(sync_update)

    return updates

# TODO: do_merge_attributes ?

def do_merge_attributes(matches, parsers, updates, settings):
    if settings.do_attributes:
        raise NotImplementedError("Do Merge Attributes not implemented")

def do_merge_prod(matches, parsers, updates, settings):
    """For a given list of matches, return a description of updates required to merge them."""

    if Registrar.DEBUG_TRACE:
        import pudb; pudb.set_trace()

    if settings.do_variations:
        updates.variation = UpdateNamespace()

    if not settings['do_sync']:
        return

    sync_handles = settings.sync_handles_prod

    if Registrar.DEBUG_UPDATE:
        Registrar.register_message("sync_handles: %s" % repr(sync_handles))

    for _, match in enumerate(matches.globals):
        if Registrar.DEBUG_CATS or Registrar.DEBUG_VARS:
            Registrar.register_message("processing match: %s" %
                                       match.tabulate())
        m_object = match.m_object
        s_object = match.s_object

        sync_update = settings.syncupdate_class_prod(m_object, s_object)

        # , "gcs %s is not variation but object is" % repr(gcs)
        assert not m_object.is_variation
        # , "gcs %s is not variation but object is" % repr(gcs)
        assert not s_object.is_variation

        sync_update.update(sync_handles)

        # print sync_update.tabulate()

        if settings.do_categories:
            sync_update.simplify_sync_warning_value_listed('product_categories', ['term_id'])

        if settings.do_images:
            sync_update.simplify_sync_warning_value_listed('attachment_objects', ['id', 'title', 'source_url'])

        # TODO: settings.do_attributes
        # if settings.do_attributes:
        #     sync_update.simplify_sync_warning_value_listed('attributes', ['term_id'])

        # Assumes that GDrive is read only, doesn't care about master
        # updates
        if not sync_update.s_updated:
            continue

        if Registrar.DEBUG_UPDATE:
            Registrar.register_message("sync updates:\n%s" %
                                       sync_update.tabulate())

        if sync_update.s_updated and sync_update.s_deltas:
            updates.delta_slave.append(sync_update)

        if not sync_update.important_static:
            updates.problematic.append(sync_update)
            continue

        if sync_update.s_updated:
            updates.slave.append(sync_update)

    if settings['auto_create_new']:
        for new_prod_count, new_prod_match in enumerate(matches.slaveless):

            m_object = new_prod_match.m_object
            Registrar.register_message(
                "will create product %d: %s" % (
                    new_prod_count, m_object.identifier
                )
            )
            sync_update = settings.syncupdate_class_prod(m_object)
            sync_update.update(sync_handles)
            updates.category.new_slaves.append(
                sync_update
            )

def do_merge_var(matches, parsers, updates, settings):
    if not settings['do_variations']:
        return

    sync_handles = settings.sync_handles_var

    if matches.variation.duplicate:
        exc = UserWarning(
            "variations couldn't be synchronized because of ambiguous SKUs:%s"
            % '\n'.join(map(str, matches.variation.duplicate)))
        Registrar.register_error(exc)
        raise exc

    for var_match_count, var_match in enumerate(matches.variation.globals):
        # print "processing var_match: %s" % var_match.tabulate()
        m_object = var_match.m_object
        s_object = var_match.s_object

        sync_update = settings.syncupdate_class_var(m_object, s_object)

        sync_update.update(sync_handles)

        if settings.do_images:
            # TODO: this might not be the right handle for variation image
            sync_update.simplify_sync_warning_value_listed('attachment_objects', ['id'])

        # Assumes that GDrive is read only, doesn't care about master
        # updates
        if not sync_update.s_updated:
            continue

        if Registrar.DEBUG_VARS:
            Registrar.register_message("var update %d:\n%s" % (
                var_match_count, sync_update.tabulate()))

        if not sync_update.important_static:
            updates.variation.problematic.append(sync_update)
            continue

        if sync_update.s_updated:
            updates.variation.slave.append(sync_update)

    for var_match_count, var_match in enumerate(
            matches.variation.slaveless):
        assert var_match.has_no_slave
        m_object = var_match.m_object

        # sync_update = SyncUpdateVarWoo(m_object, None)

        # sync_update.update()

        if Registrar.DEBUG_VARS:
            Registrar.register_message("var create %d:\n%s" % (
                var_match_count, m_object.identifier))

        # TODO: figure out which attribute terms to add

    for var_match_count, var_match in enumerate(
            matches.variation.masterless):
        assert var_match.has_no_master
        s_object = var_match.s_object

        # sync_update = SyncUpdateVarWoo(None, s_object)

        # sync_update.update()

        if Registrar.DEBUG_VARS:
            Registrar.register_message("var delete: %d:\n%s" % (
                var_match_count, s_object.identifier))

        # TODO: figure out which attribute terms to delete

    if settings['auto_create_new']:
        # TODO: auto create new variations
        raise NotImplementedError()

def do_report_images(reporters, matches, updates, parsers, settings):
    if not settings.get('do_report'):
        return reporters

    Registrar.register_progress("Write Images Report")

    do_img_sync_group(reporters.img, matches, updates, parsers, settings)

    if reporters.img:
        reporters.img.write_document_to_file('img', settings.rep_img_path)

    return reporters

def do_report_categories(reporters, matches, updates, parsers, settings):
    if not settings.get('do_report'):
        return reporters

    Registrar.register_progress("Write Categories Report")

    do_cat_sync_gruop(reporters.cat, matches, updates, parsers, settings)

    if reporters.cat:
        reporters.cat.write_document_to_file('cat', settings.rep_cat_path)

    return reporters

# TODO: do_report_attributes ?

def do_report_attributes(reporters, matches, updates, parsers, settings):
    if settings.do_attributes:
        raise NotImplementedError("Do Report Attributes not implemented")

def do_report(reporters, matches, updates, parsers, settings):
    """ Write report of changes to be made. """

    if not settings.get('do_report'):
        return reporters

    Registrar.register_progress("Write Report")

    do_main_summary_group(
        reporters.main, matches, updates, parsers, settings
    )
    do_delta_group(
        reporters.main, matches, updates, parsers, settings
    )
    do_sync_group(
        reporters.main, matches, updates, parsers, settings
    )
    do_var_sync_group(
        reporters.main, matches, updates, parsers, settings
    )

    if reporters.main:
        reporters.main.write_document_to_file('main', settings.rep_main_path)

    if settings.get('report_matching'):
        Registrar.register_progress("Write Matching Report")

        do_matches_summary_group(
            reporters.match, matches, updates, parsers, settings
        )
        do_matches_group(
            reporters.match, matches, updates, parsers, settings
        )
        if settings.do_variations:
            do_variation_matches_group(
                reporters.match, matches, updates, parsers, settings
            )
        if settings.do_categories:
            do_category_matches_group(
                reporters.match, matches, updates, parsers, settings
            )

        if reporters.match:
            reporters.match.write_document_to_file(
                'match', settings.rep_match_path)

    return reporters

def do_report_post(reporters, results, settings):
    """ Reports results from performing updates."""
    # raise NotImplementedError()
    if settings.get('do_report'):
        Registrar.register_progress("Write Post Report")

        do_post_summary_group(reporters.post, results, settings)
        do_failures_group(reporters.post, results, settings)
        do_successes_group(reporters.post, results, settings)
        if reporters.post:
            reporters.post.write_document_to_file(
                'post', settings.rep_post_path)

def handle_failed_update(update, results, exc, settings, source=None):
    """Handle a failed update."""
    fail = (update, exc)
    if source == settings.master_name:
        pkey = update.master_id
        results.fails_master.append(fail)
    elif source == settings.slave_name:
        pkey = update.slave_id
        results.fails_slave.append(fail)
    else:
        pkey = ''
    Registrar.register_error(
        "ERROR UPDATING %s (%s): %s\n%s\n%s" % (
            source or '',
            pkey,
            repr(exc),
            update.tabulate(),
            traceback.format_exc()
        )
    )

    if Registrar.DEBUG_TRACE:
        import pudb; pudb.set_trace()

def usr_prompt_continue(settings):
    try:
        raw_in = input("\n".join([
            "Please read reports and then make your selection",
            " - press Enter to continue and perform updates",
            " - press s to skip updates",
            " - press c to cancel",
            "..."
        ]))
    except SyntaxError:
        raw_in = ""
    if raw_in == 's':
        return 's'
    if raw_in == 'c':
        raise SystemExit

# TODO: collapse upload_new functions
def upload_new_images_slave(parsers, results, settings, client, new_updates):

    if not (new_updates and settings['update_slave']):
        return

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(new_updates), items_plural='new %s' % client.endpoint_plural
        )

    # sync_handles = settings.sync_handles_img

    update_count = 0

    while new_updates:

        sync_update = new_updates.pop(0)

        core_data = sync_update.get_slave_updates()

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "new %s (core format) %s" % (
                    client.endpoint_singular,
                    core_data
                )
           )

        update_count += 1
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(update_count)

        try:
            response = client.create_item_core(core_data)
            response_api_data = response.json()
        except BaseException as exc:
            handle_failed_update(
                sync_update, results, exc, settings, settings.slave_name
            )
            continue
        if client.page_nesting:
            response_api_data = response_api_data[client.endpoint_singular]

        response_gen_object = parsers.slave.analyse_api_image_raw(response_api_data)

        # TODO: fix constantly re-uploading images

        if Registrar.DEBUG_IMG:
            Registrar.register_message(
                "image being updated with parser data: %s"
                % pformat(response_gen_object))

        sync_update.set_new_s_object_gen(response_gen_object)
        sync_update.old_m_object_gen.update(response_gen_object)

        results.successes.append(sync_update)

# TODO: collapse upload_changes functions
def upload_image_changes_slave(parsers, results, settings, client, change_updates):

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(change_updates), items_plural='%s updates' % client.endpoint_singular
        )

    if not settings['update_slave']:
        return

    # if Registrar.DEBUG_TRACE:
    #     import pudb; pudb.set_trace()

    for count, sync_update in enumerate(change_updates):
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(count)

        if not sync_update.s_updated:
            continue

        try:
            pkey = sync_update.slave_id
            changes = sync_update.get_slave_updates()
            response_raw = client.upload_changes_core(pkey, changes)
            response_api_data = response_raw.json()
            if client.page_nesting:
                response_api_data = response_api_data[client.endpoint_singular]
        except Exception as exc:
            handle_failed_update(
                sync_update, results, exc, settings, settings.slave_name
            )
            continue

        if response_api_data['id'] != pkey:
            # if pkey has changed since update, i.e. a new item was uploaded
            response_gen_object = parsers.slave.analyse_api_image_raw(response_api_data)
        else:
            response_core_data = settings.coldata_class_img.translate_data_from(
                response_api_data, settings.coldata_img_target
            )
            response_gen_data = settings.coldata_class_img.translate_data_to(
                response_core_data, settings.coldata_gen_target_write
            )
            sync_update.old_s_object_gen.update(response_gen_data)
            response_gen_object = sync_update.old_s_object_gen

        sync_update.set_new_s_object_gen(response_gen_object)
        sync_update.old_m_object_gen.update(response_gen_object)

        results.successes.append(sync_update)

def do_updates_images_master(updates, parsers, results, settings):
    for update in updates.image.master:
        old_master_id = update.master_id
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "performing update < %5s | %5s > = \n%100s, %100s " %
                (update.master_id, update.slave_id,
                 str(update.old_m_object), str(update.old_s_object)))
        if not old_master_id in parsers.master.attachments:
            exc = UserWarning(
                "couldn't fine pkey %s in parsers.master.attachments" %
                update.master_id)
            Registrar.register_error(exc)
            continue
        parsers.master.attachments[old_master_id].update(
            update.get_master_updates_native()
        )

def do_updates_images_slave(updates, parsers, results, settings):
    """Perform a list of updates on attachments."""

    results.image = ResultsNamespace()
    results.image.new = ResultsNamespace()

    sync_client_class = settings.slave_img_sync_client_class
    sync_client_args = settings.slave_img_sync_client_args

    # updates in which an item is modified
    change_updates = updates.image.slave
    if settings.do_problematic:
        change_updates += updates.image.problematic
    # updates in which a new item is created
    new_updates = []
    if settings['auto_create_new']:
        new_updates += updates.image.new_slaves
    else:
        for update in new_updates:
            new_item_api = update.get_slave_updates_native()
            exc = UserWarning("{0} needs to be created: {1}".format(
                sync_client_class.endpoint_singular, new_item_api
            ))
            Registrar.register_warning(exc)
    Registrar.register_progress("Changing {1} {0} and creating {2} {0}".format(
        sync_client_class.endpoint_plural, len(change_updates), len(new_updates)
    ))

    if not (new_updates or change_updates):
        return

    if settings['ask_before_update']:
        if usr_prompt_continue(settings) == 's':
            return

    with sync_client_class(**sync_client_args) as client:
        if Registrar.DEBUG_IMG:
            Registrar.register_message("created img client")

        if new_updates:
            # create attachments that do not yet exist on slave
            upload_new_images_slave(
                parsers, results.image.new, settings, client, new_updates
            )

        if change_updates:
            upload_image_changes_slave(
                parsers, results.image, settings, client, change_updates
            )

def upload_new_categories_slave(parsers, results, settings, client, new_updates):
    """
    Create new categories in client in an order which creates parents first.
    """
    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(new_updates), items_plural='new %s' % client.endpoint_plural
        )

    if not (new_updates and settings['update_slave']):
        return

    sync_handles = settings.sync_handles_cat

    update_count = 0

    while new_updates:

        sync_update = new_updates.pop(0)
        new_object_gen = sync_update.old_m_object_gen

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "new %s %s" % (
                    client.endpoint_singular,
                    new_object_gen
                )
           )

        # make sure parent updates are done before children

        if new_object_gen.parent:
            remaining_m_objects = set([
                sync_update.old_m_object_gen for update_ in new_updates
            ])
            parent = new_object_gen.parent
            if not parent.is_root and parent in remaining_m_objects:
                new_updates.append(sync_update)
                continue


        # have to refresh sync_update to get parent wpid since parente wpid is populated in do_updates_categories_master
        sync_update.set_old_m_object_gen(sync_update.old_m_object)
        sync_update.update(sync_handles)

        core_data = sync_update.get_slave_updates()

        if Registrar.DEBUG_UPDATE:
            Registrar.register_message("uploading new category (api format): %s" % pformat(core_data))

        update_count += 1
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(update_count)

        try:
            response = client.create_item_core(core_data)
            response_api_data = response.json()
        except BaseException as exc:
            handle_failed_update(
                sync_update, results, exc, settings, settings.slave_name
            )
            continue
        if client.page_nesting:
            response_api_data = response_api_data[client.endpoint_singular]

        response_gen_object = parsers.slave.process_api_category_raw(response_api_data)

        sync_update.set_new_s_object_gen(response_gen_object)

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "%s being updated with parser data: %s" % (
                    client.endpoint_singular,
                    pformat(response_gen_object)
                )
            )

        sync_update.old_m_object_gen.update(response_gen_object)

        results.successes.append(sync_update)

def upload_category_changes_slave(parsers, results, settings, client, change_updates):
    """
    Upload a list of category changes
    """

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(change_updates), items_plural='%s updates' % client.endpoint_singular
        )

    if not settings['update_slave']:
        return

    if Registrar.DEBUG_TRACE:
        import pudb; pudb.set_trace()

    for count, sync_update in enumerate(change_updates):
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(count)

        if not sync_update.s_updated:
            continue

        try:
            pkey = sync_update.slave_id
            changes = sync_update.get_slave_updates_native()
            # TODO: does this need replacing?
            # if 'image' in changes:
            #     # quick hack to stop syncing bad images
            #     if 'src' in changes['image']:
            #         del changes['image']
            response_raw = client.upload_changes(pkey, changes)
            response_api_data = response_raw.json()
        except Exception as exc:
            handle_failed_update(
                sync_update, results, exc, settings, settings.slave_name
            )
            continue

        response_core_data = settings.coldata_class_cat.translate_data_from(
            response_api_data, settings.coldata_cat_target
        )
        response_gen_data = settings.coldata_class_cat.translate_data_to(
            response_core_data, settings.coldata_gen_target_write
        )

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "%s being updated with parser data: %s" % (
                    client.endpoint_singular,
                    pformat(response_gen_data)
                )
            )

        sync_update.old_s_object_gen.update(response_gen_data)
        sync_update.set_new_s_object_gen(sync_update.old_s_object_gen)
        sync_update.old_m_object_gen.update(response_gen_data)

        results.successes.append(sync_update)

def do_updates_categories_master(updates, parsers, results, settings):
    for update in updates.category.master:
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "performing update < %5s | %5s > = \n%100s, %100s " %
                (
                    update.master_id, update.slave_id,
                    str(update.old_m_object), str(update.old_s_object))
                )
        if not update.master_id in parsers.master.categories:
            exc = UserWarning(
                "couldn't fine pkey %s in parsers.master.categories" %
                update.master_id)
            Registrar.register_error(exc)
            continue
        parsers.master.categories[update.master_id].update(
            update.get_master_updates_native()
        )

def do_updates_categories_slave(updates, parsers, results, settings):
    """Perform a list of updates on categories."""
    if not hasattr(updates, 'category'):
        return

    results.category = ResultsNamespace()
    results.category.new = ResultsNamespace()

    sync_client_class = settings.slave_cat_sync_client_class
    sync_client_args = settings.slave_cat_sync_client_args

    # updates in which an item is modified
    change_updates = updates.category.slave
    if settings.do_problematic:
        change_updates += updates.category.problematic
    # updates in which a new item is created
    new_updates = []
    if settings['auto_create_new']:
        new_updates += updates.category.new_slaves
    else:
        for update in new_updates:
            new_item_api = update.get_slave_updates_native()
            exc = UserWarning("{0} needs to be created: {1}".format(
                sync_client_class.endpoint_plural, new_item_api
            ))
            Registrar.register_warning(exc)
    Registrar.register_progress("Changing {1} {0} and creating {2} {0}".format(
        sync_client_class.endpoint_plural, len(change_updates), len(new_updates)
    ))

    if not (new_updates or change_updates):
        return

    if settings['ask_before_update']:
        if usr_prompt_continue(settings) == 's':
            return

    with sync_client_class(**sync_client_args) as client:
        if Registrar.DEBUG_CATS:
            Registrar.register_message("created cat client")

        if new_updates:
            # create categories that do not yet exist on slave
            upload_new_categories_slave(
                parsers, results.category.new, settings, client, new_updates
            )

        if change_updates:
            upload_category_changes_slave(
                parsers, results.category, settings, client, change_updates
            )

# TODO: do_updates_attributes_master ?
def do_updates_attributes_master(updates, parsers, results, settings):
    if settings.do_attributes:
        raise NotImplementedError("Do Updates Attributes Master not implemented")

# TODO: do_updates_attributes_slave ?
def do_updates_attributes_slave(updates, parsers, results, settings):
    if settings.do_attributes:
        raise NotImplementedError("Do Updates Attributes Slave not implemented")

def upload_new_products(parsers, results, settings, client, new_updates):
    """
    Create new products in client in an order which creates parents first.
    """
    raise NotImplementedError()

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(new_updates), items_plural='new %s' % client.endpoint_plural
        )

    if not (new_updates and settings['update_slave']):
        return

    sync_handles = settings.sync_handles_prod

    update_count = 0

    while new_updates:

        sync_update = new_updates.pop(0)
        new_object_gen = sync_update.old_m_object_gen

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "new %s %s" % (
                    client.endpoint_singular,
                    new_object_gen
                )
           )

        # have to refresh sync_update to get parent wpid since parente wpid is populated in do_updates_categories_master
        sync_update.set_old_m_object_gen(sync_update.old_m_object)
        sync_update.update(sync_handles)

        core_data = sync_update.get_slave_updates()

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "new %s (core format) %s" % (
                    client.endpoint_singular,
                    core_data
                )
           )

        update_count += 1
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(update_count)

        try:
            response = client.create_item_core(core_data)
            response_api_data = response.json()
        except BaseException as exc:
            handle_failed_update(
                sync_update, results, exc, settings, settings.slave_name
            )
            continue
        if client.page_nesting:
            response_api_data = response_api_data[client.endpoint_singular]

        response_gen_object = parsers.slave.analyse_api_obj(response_api_data)

        sync_update.set_new_s_object_gen(response_gen_object)

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "%s being updated with parser data: %s" % (
                    client.endpoint_singular,
                    pformat(response_gen_object)
                )
            )

        sync_update.old_m_object_gen.update(response_gen_object)

        results.successes.append(sync_update)

def upload_product_changes(parsers, results, settings, client, change_updates):

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(change_updates), items_plural='%s updates' % client.endpoint_singular
        )

    if not settings['update_slave']:
        return

    for count, sync_update in enumerate(change_updates):
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(count)

        if not sync_update.s_updated:
            continue

        try:
            pkey = sync_update.slave_id
            changes = sync_update.get_slave_updates_native()
            response_raw = client.upload_changes(pkey, changes)
            response_api_data = response_raw.json()
        except Exception as exc:
            handle_failed_update(
                sync_update, results, exc, settings, settings.slave_name
            )
            continue

        response_core_data = settings.coldata_class.translate_data_from(
            response_api_data, settings.coldata_cat_target
        )
        response_gen_data = settings.coldata_class.translate_data_to(
            response_core_data, settings.coldata_gen_target_write
        )

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "%s being updated with parser data: %s" % (
                    client.endpoint_singular,
                    pformat(response_gen_data)
                )
            )

        sync_update.old_s_object_gen.update(response_gen_data)
        sync_update.set_new_s_object_gen(sync_update.old_s_object_gen)
        sync_update.old_m_object_gen.update(response_gen_data)

        results.successes.append(sync_update)

def do_updates_prod(updates, parsers, settings, results):
    """
    Update products in slave.
    """
    # updates in which an item is modified

    results.new = ResultsNamespace()
    slave_client_class = settings.slave_upload_client_class
    slave_client_args = settings.slave_upload_client_args

    change_updates = updates.slave
    if settings.do_problematic:
        change_updates += updates.problematic
    # updates in which a new item is created
    new_updates = []
    if settings['auto_create_new']:
        new_updates += updates.new_slaves
    else:
        for update in new_updates:
            new_item_api = update.get_slave_updates_native()
            exc = UserWarning("{0} needs to be created: {1}".format(
                slave_client_class.endpoint_singular, new_item_api
            ))
            Registrar.register_warning(exc)
    Registrar.register_progress("Changing {1} {0} and creating {2} {0}".format(
        slave_client_class.endpoint_plural, len(change_updates), len(new_updates)
    ))

    if not (new_updates or change_updates):
        return

    if settings['ask_before_update']:
        if usr_prompt_continue(settings) == 's':
            return


    with slave_client_class(**slave_client_args) as client:
        if new_updates:
            upload_new_products(
                parsers, results, settings, client, new_updates
            )
        if change_updates:
            upload_product_changes(
                parsers, results, settings, client, change_updates
            )

def do_updates_var(updates, parsers, settings, results):
    raise NotImplementedError()

    change_updates = updates.variation.slave
    if settings.do_problematic:
        change_updates += updates.variation.problematic

    if not change_updates:
        return


def main(override_args=None, settings=None):
    """Main function for generator."""
    if not settings:
        settings = SettingsNamespaceProd()
    settings.init_settings(override_args)

    settings.init_dirs()

    ########################################
    # Create Product Parser object
    ########################################

    parsers = ParserNamespace()
    populate_master_parsers(parsers, settings)

    check_warnings(settings)

    if settings.schema_is_woo and settings.do_images:
        process_images(settings, parsers)

    if parsers.master.objects:
        export_master_parser(settings, parsers)

    if settings.master_and_quit:
        sys.exit(ExitStatus.success)

    populate_slave_parsers(parsers, settings)

    if parsers.slave.objects:
        cache_api_data(settings, parsers)

    matches = MatchNamespace(
        index_fn=ProductMatcher.product_index_fn
    )
    updates = UpdateNamespace()
    reporters = ReporterNamespace()
    results = ResultsNamespace()

    if settings.do_images:
        do_match_images(parsers, matches, settings)
        do_merge_images(matches, parsers, updates, settings)
        do_report_images(
            reporters, matches, updates, parsers, settings
        )
        Registrar.register_message(
            "pre-sync summary: \n%s" % reporters.img.get_summary_text()
        )
        check_warnings(settings)
        if not settings.report_and_quit:
            do_updates_images_master(updates, parsers, results, settings)
            try:
                do_updates_images_slave(updates, parsers, results, settings)
            except (SystemExit, KeyboardInterrupt) as exc:
                Registrar.register_error(exc)
                return reporters, results

    if settings.do_categories:

        do_match_categories(parsers, matches, settings)
        do_merge_categories(matches, parsers, updates, settings)
        do_report_categories(
            reporters, matches, updates, parsers, settings
        )
        Registrar.register_message(
            "pre-sync summary: \n%s" % reporters.cat.get_summary_text()
        )
        check_warnings(settings)
        if not settings.report_and_quit:
            do_updates_categories_master(updates, parsers, results, settings)
            try:
                do_updates_categories_slave(updates, parsers, results, settings)
            except (SystemExit, KeyboardInterrupt) as exc:
                Registrar.register_error(exc)
                return reporters, results

    if settings.do_attributes:
        Registrar.register_error(NotImplementedError("Functions past this point have not been completed"))
        return reporters, results

        do_match_attributes(parsers, matches, settings)
        do_merge_attributes(matches, parsers, updates, settings)
        do_report_attributes(
            reporters, matches, updates, parsers, settings
        )
        Registrar.register_message(
            "pre-sync summary: \n%s" % reporters.attr.get_summary_text()
        )
        check_warnings(settings)
        if not settings.report_and_quit:
            do_updates_attributes_master(updates, parsers, results, settings)
            try:
                do_updates_attributes_slave(updates, parsers, results, settings)
            except (SystemExit, KeyboardInterrupt) as exc:
                Registrar.register_error(exc)
                return reporters, results

    do_match_prod(parsers, matches, settings)
    do_merge_prod(matches, parsers, updates, settings)
    do_merge_var(matches, parsers, updates, settings)
    # check_warnings(settings)
    do_report(reporters, matches, updates, parsers, settings)

    if settings.report_and_quit:
        sys.exit(ExitStatus.success)

    check_warnings(settings)

    Registrar.register_message(
        "pre-sync summary: \n%s" % reporters.main.get_summary_text()
    )

    try:
        do_updates_prod(updates, parsers, settings, results)
    except (SystemExit, KeyboardInterrupt) as exc:
        Registrar.register_error(exc)
        return reporters, results
    if settings['do_variations']:

        Registrar.register_error(NotImplementedError("Functions past this point have not been completed"))
        return reporters, results

        try:
            do_updates_var(updates, parsers, settings, results)
        except (SystemExit, KeyboardInterrupt) as exc:
            Registrar.register_error(exc)
            return reporters, results
    do_report_post(reporters, results, settings)

    Registrar.register_message(
        "post-sync summary: \n%s" % reporters.post.get_summary_text()
    )

    #########################################
    # Display reports
    #########################################

    Registrar.register_progress("Displaying reports")

    if settings.do_report:
        if settings['rep_web_path']:
            shutil.copyfile(settings.rep_main_path, settings['rep_web_path'])
            if settings['web_browser']:
                os.environ['BROWSER'] = settings['web_browser']
                # print "set browser environ to %s" % repr(web_browser)
            # print "moved file from %s to %s" % (settings.rep_main_path,
            # repWeb_path)

            webbrowser.open(settings['rep_web_link'])
    else:
        print "open this link to view report %s" % settings['rep_web_link']


def catch_main(override_args=None):
    """Run the main function within a try statement and attempt to analyse failure."""
    file_path = __file__
    cur_dir = os.getcwd() + '/'
    if file_path.startswith(cur_dir):
        file_path = file_path[len(cur_dir):]
    override_args_repr = ''
    if override_args is not None:
        override_args_repr = ' '.join(override_args)

    full_run_str = "%s %s %s" % (
        str(sys.executable), str(file_path), override_args_repr)

    settings = SettingsNamespaceProd()

    status = 0
    try:
        main(settings=settings, override_args=override_args)
    except SystemExit:
        status = ExitStatus.failure
    except KeyboardInterrupt:
        pass
    except BaseException as exc:
        status = 1
        if isinstance(exc, UserWarning):
            status = 65
        elif isinstance(exc, IOError):
            status = 74
            print( "cwd: %s" % os.getcwd() )
        elif exc.__class__ in ["ReadTimeout", "ConnectionError", "ConnectTimeout", "ServerNotFoundError"]:
            status = 69  # service unavailable

        if status:
            Registrar.register_error(traceback.format_exc())
            Registrar.raise_exception(exc)

    with io.open(settings.log_path, 'w+', encoding='utf8') as log_file:
        for source, messages in Registrar.get_message_items(1).items():
            print source
            log_file.writelines([SanitationUtils.coerce_unicode(source)])
            log_file.writelines([
                SanitationUtils.coerce_unicode(message) for message in messages
            ])
            for message in messages:
                pprint(message, indent=4, width=80, depth=2)

    #########################################
    # zip reports
    #########################################

    files_to_zip = [
        settings.rep_fail_master_csv_path, settings.rep_fail_slave_csv_path, settings.rep_main_path
    ]

    with zipfile.ZipFile(settings.zip_path, 'w') as zip_file:
        for file_to_zip in files_to_zip:
            try:
                os.stat(file_to_zip)
                zip_file.write(file_to_zip)
            except BaseException:
                pass
        Registrar.register_message('wrote file %s' % zip_file.filename)

    # print "\nexiting with status %s \n" % status
    if status:
        print "re-run with: \n%s" % full_run_str
    else:
        Registrar.register_message("re-run with:\n%s" % full_run_str)

    sys.exit(status)


if __name__ == '__main__':
    catch_main()
