"""
Generator extracts data from a master and slave database of product
information, then synchronizes that information on several layers.
In a typical configuration, the master is a spreadsheet of product information
in the generator format, which is a custom heirarchical representation of
product and category data designed for minimal redundancy.
The generator format is a hybrid of a tree structure with a flat-file table.
Product titles and SKUs are generated by concatenating the name and code
fragments of a product's ancestors in a tree.

These are the steps that generator uses to synchronize products:
 - The master information is optionally downloaded from Google Drive if the
 `--download-master` flag is set, or it is read from local csv files.
 - Master information is parsed into a master `CSVParse` object which contains
 several `Import` objects which correspond to products, variations, images,
 categories and other objects within the database.
 - The master information is exported as a set of CSV file which is compatible
 with the WooCommerce Product CSV Import Suite plugin. At this point the
 program can be terminated if the `--master-and-quit` command line flag is set.
 - Slave product information is optionally downloaded from the the selected API
 and parsed in a similar way to master if the `--download-slave` flag is set,
 otherwise it is parsed from local json files
 - The objectes from these parser objects are matched with each other into a
 collection of `match` objects which need special attention when the `matcher`
 objects encounter duplicates. Matching has to be done separately for images,
 categories, products and product variations in this order since each of these
 layers are inter-dependent.
 - The `match` objects are analysed and a collection of `sync_update` objects
 are generated which describe how to update the databases so that they match.
 This process is referred to as merging.
 - Before any updates are done, some HTML reports are generated and the user
 is asked for confirmation before continuing.
 - The updates are then carried out on the slave database using a `sync_client`
 for each layer.

"""

from __future__ import absolute_import

import io
import itertools
import os
import shutil
import sys
import time
import traceback
import webbrowser
import zipfile
import pudb
from bisect import insort
from collections import OrderedDict
from copy import copy
from pprint import pformat, pprint

from requests.exceptions import ConnectionError, ConnectTimeout, ReadTimeout

from exitstatus import ExitStatus
from six import string_types

from .images import process_images
from .matching import (AttacheeSkuMatcher, AttacheeTitleMatcher,
                       AttachmentIDMatcher, CategoryMatcher,
                       CategoryTitleMatcher, ImageMatcher, Match, MatchList,
                       ProductMatcher, VariationMatcher)
from .namespace.core import (MatchNamespace, ParserNamespace, ResultsNamespace,
                             UpdateNamespace)
from .namespace.prod import SettingsNamespaceProd
from .parsing.api import ApiParseWoo
from .parsing.dyn import CsvParseDyn
from .parsing.special import CsvParseSpecial
from .parsing.woo import WooCatList
from .utils import (ProgressCounter, Registrar, SanitationUtils, SeqUtils,
                    TimeUtils)
from .utils.reporter import (ReporterNamespace, do_cat_sync_gruop,
                             do_category_matches_group, do_delta_group,
                             do_duplicates_group, do_duplicates_summary_group,
                             do_failures_group, do_img_sync_group,
                             do_main_summary_group, do_matches_group,
                             do_matches_summary_group, do_post_summary_group,
                             do_successes_group, do_sync_group,
                             do_var_sync_group, do_variation_matches_group)


# to run full sync test on TT
"""
python -m woogenerator.generator \
    --testmode --schema "TT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --download-master --download-slave \
    --do-categories --do-images --do-specials \
    --do-sync --update-slave --do-problematic --auto-create-new \
    --ask-before-update \
    -vvv --debug-trace
"""

# To run full sync test on VT
"""
python -m woogenerator.generator \
    --testmode --schema "VT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --local-test-config 'generator_config_test_vt.yaml' \
    --download-master --download-slave \
    --do-categories --do-specials \
    --do-images --do-resize-images \
    --do-sync --update-slave --do-problematic --auto-create-new \
    --ask-before-update \
    -vvv --debug-trace
"""

# TT just master and quit
"""
python -m woogenerator.generator \
    --testmode --schema "TT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --download-master --download-slave \
    --do-categories --do-images --do-variations --master-and-quit \
    --do-specials --specials-mode 'auto_next' \
    -vvv --debug-trace
"""

# VT just master and quit
"""
python -m woogenerator.generator \
    --testmode --schema "VT" \
    --local-work-dir '/Users/derwent/Documents/woogenerator/' \
    --local-test-config 'generator_config_test_vt.yaml' \
    --download-master --download-slave \
    --do-categories --do-images --do-variations --master-and-quit \
    --do-specials --specials-mode 'all_future' \
    -vvv --debug-trace
"""


def timediff(settings):
    """Return time elapsed since start."""
    return time.time() - settings.start_time


def check_warnings(settings):
    """
    Check if there have been any errors or warnings registered in Registrar.

    Raise approprriate exceptions if needed
    """
    if Registrar.errors:
        print("there were some urgent errors "
              "that need to be reviewed before continuing")
        Registrar.print_message_dict(0)
        usr_prompt_continue(settings)
        if Registrar.DEBUG_TRACE:
            pudb.set_trace()

    elif Registrar.warnings:
        print("there were some warnings that should be reviewed")
        Registrar.print_message_dict(1)


def populate_master_parsers(parsers, settings):
    """Create and populates the various parsers."""
    Registrar.register_message('schema: %s, woo_schemas: %s' % (
        settings.schema, settings.woo_schemas
    ))

    parsers.dyn = CsvParseDyn()
    parsers.special = CsvParseSpecial()

    if Registrar.DEBUG_GEN:
        Registrar.register_message(
            "master_download_client_args: %s" %
            settings.master_download_client_args)

    with settings.master_download_client_class(
        **settings.master_download_client_args
    ) as client:

        if settings.schema_is_woo:
            if settings.do_dyns:
                Registrar.register_message("analysing dprc rules")
                client.analyse_remote(
                    parsers.dyn,
                    data_path=settings.dprc_path,
                    gid=settings.dprc_gid
                )
                settings.dprc_rules = parsers.dyn.taxos

                Registrar.register_message("analysing dprp rules")
                parsers.dyn.clear_transients()
                client.analyse_remote(
                    parsers.dyn,
                    data_path=settings.dprp_path,
                    gid=settings.dprp_gid
                )
                settings.dprp_rules = parsers.dyn.taxos

            if settings.do_specials:
                Registrar.register_message("analysing specials")
                client.analyse_remote(
                    parsers.special,
                    data_path=settings.specials_path,
                    gid=settings.spec_gid
                )
                if Registrar.DEBUG_SPECIAL:
                    Registrar.register_message(
                        "all specials: %s" % parsers.special.tabulate()
                    )

                settings.special_rules = parsers.special.rules

                settings.current_special_groups = \
                    parsers.special.determine_current_spec_grps(
                        specials_mode=settings.specials_mode,
                        current_special=settings.current_special
                    )

                if settings.current_special_groups:
                    Registrar.register_message(
                        "current_special_groups: \n%s" % (
                            parsers.special.taxo_container.container(
                                settings.current_special_groups
                            ).tabulate()
                        )
                    )
                else:
                    Registrar.register_warning(
                        (
                            "No special groups were found, "
                            "here are the latest specials: \n%s"
                        ) % (
                            parsers.special.taxo_container.container(
                                parsers.special.last_5()
                            ).tabulate()
                        )
                    )

        master_parser_args = settings.master_parser_args

        master_mod_dt = TimeUtils.current_datetime()
        if os.path.exists(settings.master_path):
            master_mod_ts = max(
                os.path.getmtime(settings.master_path),
                os.path.getctime(settings.master_path)
            )
            master_mod_dt = TimeUtils.inform_datetime(
                TimeUtils.timestamp2datetime(master_mod_ts),
                TimeUtils._local_tz
            )
        elif hasattr(client, 'get_gm_modtime'):
            master_mod_dt = TimeUtils.localize_datetime(
                client.get_gm_modtime(settings.gen_gid),
                TimeUtils._local_tz
            )

        master_parser_args['defaults'].update({
            'modified_local': master_mod_dt,
            'modified_gmt': TimeUtils.localize_datetime(
                master_mod_dt, TimeUtils.utc_tz
            )
        })

        parsers.master = settings.master_parser_class(
            **master_parser_args
        )

        Registrar.register_progress("analysing master product data")

        analysis_kwargs = {
            'data_path': settings.master_path,
            'gid': settings.gen_gid,
            'limit': settings['master_parse_limit']
        }
        if Registrar.DEBUG_PARSER:
            Registrar.register_message("analysis_kwargs: %s" % analysis_kwargs)

        client.analyse_remote(parsers.master, **analysis_kwargs)

        if Registrar.DEBUG_PARSER and hasattr(
                parsers.master, 'categories_name'):
            for category_name, category_list in getattr(
                    parsers.master, 'categories_name').items():
                if len(category_list) < 2:
                    continue
                if SeqUtils.check_equal(
                        [category.namesum for category in category_list]):
                    continue
                Registrar.register_warning("bad category: %50s | %d | %s" % (
                    category_name[:50], len(category_list), str(category_list)
                ))

        return parsers


def populate_slave_parsers(parsers, settings):
    """Populate the parsers for data from the slave database."""

    parsers.slave = settings.slave_parser_class(**settings.slave_parser_args)

    slave_client_class = settings.slave_download_client_class
    slave_client_args = settings.slave_download_client_args
    slave_var_client_class = settings.slave_var_sync_client_class
    slave_var_client_args = settings.slave_var_sync_client_args

    # with ProdSyncClientWC(settings['slave_wp_api_params']) as client:

    if settings.schema_is_woo and settings.do_categories:
        Registrar.register_progress("analysing API category data")

        cat_sync_client_class = settings.slave_cat_sync_client_class
        cat_sync_client_args = settings.slave_cat_sync_client_args

        with cat_sync_client_class(**cat_sync_client_args) as client:
            client.analyse_remote_categories(
                parsers.slave,
                data_path=settings.slave_cat_path
            )

    # TODO: ignore products which are post_status = trash

    with slave_client_class(**slave_client_args) as client:

        Registrar.register_progress("analysing API product data")

        client.analyse_remote(
            parsers.slave,
            data_path=settings.slave_path,
        )

        if settings.do_variations:

            Registrar.register_progress("analysing API variation data")

            var_client = slave_var_client_class(**slave_var_client_args)

            for prod in parsers.slave.products.values():
                if not getattr(prod, 'is_variable', None):
                    continue
                parent_id = prod.api_id
                var_client.analyse_remote_variations(
                    parsers.slave,
                    parent_pkey=parent_id,
                    data_path=settings.get_slave_var_path(parent_id)
                )

    if settings.schema_is_woo and settings.do_images:
        Registrar.register_progress("analysing API image data")
        img_client_class = settings.slave_img_sync_client_class
        img_client_args = settings.slave_img_sync_client_args

        with img_client_class(**img_client_args) as client:
            client.analyse_remote_imgs(
                parsers.slave,
                data_path=settings.slave_img_path,
                skip_unattached_images=settings.skip_unattached_images
            )

    if Registrar.DEBUG_CLIENT:
        container = settings.slave_parser_class.product_container.container
        prod_list = container(parsers.slave.products.values())
        Registrar.register_message("Products: \n%s" % prod_list.tabulate())

    return parsers


def export_categories(settings, parser, csv_file, export_target):
    # category_cols = settings.coldata_class_cat.get_col_data_native(
    #     'write', target=export_target)
    category_col_names = settings.coldata_class_cat.get_col_values_native(
        'path', target=export_target)
    for col in settings.exclude_cols_cat:
        if col in category_col_names:
            del category_col_names[col]
    category_container = \
        settings.master_parser_class.category_container.container
    category_list = category_container([
        category for category in parser.categories.values()
        if category.members
    ])
    category_list.export_items(
        csv_file, category_col_names,
        coldata_target=export_target
    )


def export_master_parser(settings, parsers):
    """Export key information from master parser to csv."""
    Registrar.register_progress("Exporting Master info to disk")

    export_target = 'wc-csv'

    # Create output directory if not exist
    try:
        os.makedirs(settings.out_dir_full)
    except OSError:
        pass

    product_colnames = settings.coldata_class.get_col_values_native(
        'path', target=export_target)

    for col in settings.exclude_cols:
        if col in product_colnames:
            del product_colnames[col]

    extra_colnames = OrderedDict([
        ('title_1', 'meta:title_1'),
        ('title_2', 'meta:title_2')
    ])
    if settings.schema_is_woo and settings.do_attributes:
        extra_colnames = SeqUtils.combine_ordered_dicts(
            extra_colnames,
            settings.coldata_class.get_attribute_colnames_native(
                parsers.master.attributes, parsers.master.vattributes
            )
        )
    product_colnames = SeqUtils.combine_ordered_dicts(
        product_colnames, extra_colnames
    )

    container = parsers.master.product_container.container

    product_list = container(parsers.master.products.values())

    product_list.export_items(
        settings.fla_path, product_colnames,
        coldata_target=export_target,
        extra_colnames=extra_colnames
    )

    # TODO: stop exporting modified_gmt to spreadsheet

    if settings.schema_is_woo:
        # variations
        variation_container = \
            settings.master_parser_class.variation_container.container
        # variation_cols = \
        #     settings.coldata_class_var.get_col_data_native(
        #         'write', target='wc-csv')
        variation_col_names = \
            settings.coldata_class_var.get_col_values_native(
                'path', target=export_target)
        extra_variation_col_names = \
            settings.coldata_class_var.get_attribute_meta_colnames_native(
                parsers.master.vattributes
            )
        variation_col_names = SeqUtils.combine_ordered_dicts(
            variation_col_names, extra_variation_col_names
        )
        if settings.do_variations and parsers.master.variations:

            variation_list = variation_container(
                parsers.master.variations.values())
            variation_list.export_items(
                settings.flv_path, variation_col_names,
                coldata_target=export_target,
                extra_colnames=extra_variation_col_names
            )

            updated_variations = parsers.master.updated_variations.values()

            if updated_variations:
                updated_variations_list = variation_container(
                    updated_variations)
                updated_variations_list.export_items(
                    settings.flvu_path, variation_col_names,
                    coldata_target=export_target,
                    extra_colnames=extra_variation_col_names
                )

        # categories
        if settings.do_categories and parsers.master.categories:
            export_categories(
                settings, parsers.master, settings.cat_path, export_target)

        # specials
        if settings.do_specials and settings.current_special_id:
            special_products = parsers.master.onspecial_products.values()
            if special_products:
                special_product_list = container(special_products)
                special_product_list.export_items(
                    settings.fls_path, product_colnames,
                    coldata_target=export_target,
                    extra_colnames=extra_colnames
                )
            special_variations = parsers.master.onspecial_variations.values()
            if special_variations:
                sp_variation_list = variation_container(special_variations)
                sp_variation_list.export_items(
                    settings.flvs_path, variation_col_names,
                    coldata_target=export_target,
                    extra_colnames=extra_variation_col_names
                )

        updated_products = parsers.master.updated_products.values()
        if updated_products:
            updated_product_list = container(updated_products)
            updated_product_list.export_items(
                settings.flu_path, product_colnames,
                coldata_target=export_target,
                extra_colnames=extra_colnames
            )

            # TODO; updated variations

    Registrar.register_progress("CSV Files have been created.")


def cache_api_data(settings, parsers):
    """Export key information from slave parser to csv."""
    if not settings.download_slave:
        return
    if not settings.save_api_data:
        return

    Registrar.register_progress("Exporting Slave info to disk")
    container = settings.slave_parser_class.product_container.container
    product_list = container(parsers.slave.products.values())
    product_list.export_api_data(settings.slave_path)

    if settings.do_categories and parsers.slave.categories:
        category_container = \
            settings.slave_parser_class.category_container.container
        category_list = category_container(parsers.slave.categories.values())
        category_list.export_api_data(settings.slave_cat_path)

    if settings.do_images and parsers.slave.attachments:
        attachment_container = \
            settings.slave_parser_class.attachment_container.container
        image_list = attachment_container(parsers.slave.attachments.values())
        image_list.export_api_data(settings.slave_img_path)

    if settings.do_variations and parsers.slave.variations:
        variation_container = \
            settings.slave_parser_class.variation_container.container
        for prod in product_list:
            if not prod.is_variable:
                continue
            var_list = variation_container(prod.variations.values())
            if not var_list:
                continue
            var_list.export_api_data(settings.get_slave_var_path(prod.api_id))


def do_match_images(parsers, matches, settings):
    if Registrar.DEBUG_IMG:
        Registrar.register_message(
            "matching %d master attachments with %d slave attachments" %
            (len(parsers.master.attachments),
             len(parsers.slave.attachments)))

    matches.image = MatchNamespace(
        index_fn=ImageMatcher.image_index_fn
    )

    # TODO: fix not syncing to VuTan (see generator.todo)

    image_matcher = ImageMatcher()
    image_matcher.clear()
    slave_imgs_attachments = OrderedDict([
        (index, image) for index, image in parsers.slave.attachments.items()
        if image.attaches.has_products_categories
    ])
    master_imgs_attachments = OrderedDict([
        (index, image) for index, image in parsers.master.attachments.items()
        if image.attaches.has_products_categories
    ])
    image_matcher.process_registers(
        slave_imgs_attachments, master_imgs_attachments
    )

    matches.image.globals.add_matches(image_matcher.pure_matches)
    matches.image.masterless.add_matches(image_matcher.masterless_matches)
    matches.image.slaveless.add_matches(image_matcher.slaveless_matches)

    if Registrar.DEBUG_IMG:
        if image_matcher.pure_matches:
            Registrar.register_message("All Image matches:\n%s" % (
                '\n'.join(map(str, image_matcher.matches))))

    matches.image.valid += image_matcher.pure_matches

    if not image_matcher.duplicate_matches:
        return matches

    extra_valid_indices_m = set()
    extra_valid_indices_s = set()

    matches.image.duplicate['file_name'] = image_matcher.duplicate_matches

    filename_duplicate_indices_m = set([
        attachment.index
        for match in image_matcher.duplicate_matches
        for attachment in match.m_objects
    ])
    filename_duplicate_indices_s = set([
        attachment.index
        for match in image_matcher.duplicate_matches
        for attachment in match.s_objects
    ])

    for match in image_matcher.duplicate_matches:
        if Registrar.DEBUG_IMG or Registrar.DEBUG_TRACE:
            Registrar.register_message(
                "analysing duplicate match:\n%s" % match.tabulate()
            )
        attachee_sku_sub_matches = \
            image_matcher.find_attachee_sku_matches(match)
        for key, match in attachee_sku_sub_matches.items():
            if Registrar.DEBUG_IMG or Registrar.DEBUG_TRACE:
                Registrar.register_message(
                    "sub match %s is %s:\n%s" % (
                        key,
                        match.type,
                        match.tabulate()
                    )
                )
            if (
                match.type in ['pure', 'masterless', 'slaveless']
                or match.type == 'duplicate' and match.m_len == 1
            ):
                extra_valid_indices_m.update([
                    attachment.index for attachment in match.m_objects
                ])
                extra_valid_indices_s.update([
                    attachment.index for attachment in match.s_objects
                ])
                if match.type in ['pure', 'duplicate']:
                    matches.image.valid += [match]
                elif match.type == 'masterless':
                    try:
                        matches.image.masterless.add_matches([match])
                    except AssertionError as exc:
                        Registrar.register_warning(exc)
                elif match.type == 'slaveless':
                    try:
                        matches.image.slaveless.add_matches([match])
                    except AssertionError as exc:
                        Registrar.register_warning(exc)
            else:
                exc = UserWarning(
                    (
                        "Could not match image, most likely because multiple "
                        "images with the same name are attached to the same "
                        "SKU.\n%s"
                    ) % (
                        match.tabulate()
                    )
                )
                Registrar.register_warning(exc)

    try:
        assert \
            extra_valid_indices_m.issuperset(filename_duplicate_indices_m), \
            (
                "all master indices from filename duplicates should be "
                "contained in extra attachee match indices:\nfilename:\n"
                "%s\nattachee_indices:\n%s"
            ) % (
                filename_duplicate_indices_m,
                extra_valid_indices_m
            )
        assert \
            extra_valid_indices_s.issuperset(filename_duplicate_indices_s), \
            (
                "all slave indices from filename duplicates should be "
                "contained in extra attachee match indices:\nfilename:\n"
                "%s\nattachee_indices:\n%s"
            ) % (
                filename_duplicate_indices_s,
                extra_valid_indices_s
            )
    except AssertionError as exc:
        warn = RuntimeWarning(
            "could not match all images.\n%s\n%s" % (
                "\n".join([
                    "%s:\n%s" % (key, dup_matches.tabulate())
                    for key, dup_matches in matches.image.duplicate.items()
                ]),
                str(exc)
            )
        )
        Registrar.register_warning(warn)
        if Registrar.DEBUG_TRACE:
            pudb.set_trace()

    if Registrar.DEBUG_IMG or Registrar.DEBUG_TRACE:
        Registrar.register_message(
            "all matches:\n%s" % matches.image.tabulate())

    return matches


def do_match_categories(parsers, matches, settings):
    """
    Analyse the master and slave categories for matches.

    Categorising these matches into either
     - masterless = a category exists in slave that doesn't exist in master
     - slaveless = a category exists in master that doesn't exist in slave
     - invalid = an ambiguous match between categories
     - valid = either a category in master which matches exclusively with a
         category in slave or an ambiguous match between categories that could
         still be acted upon.

    Example of an ambiguous but valid category match:
        Master category tree:

        A - Product A
            ACA - Company A Product A
                ACARA - Company A Product A Range A
                ACARB - Company A Product A Range B

        Slave category tree (where slave is only Company A products):

        ACA - Product A
            ACARA - Product A Range A
            ACARB - Product A Range B

        so slave ACA matches with master A and ACA after master has been
        processed.
        The category tree can collapse multiple master categories into
        single slave.
    """

    if Registrar.DEBUG_CATS:
        Registrar.register_message(
            "matching %d master categories with %d slave categories" %
            (len(parsers.master.categories),
             len(parsers.slave.categories)))

    # Matching on "cat_name"

    matches.category = MatchNamespace(
        index_fn=CategoryMatcher.category_index_fn
    )

    if not(parsers.master.categories and parsers.slave.categories):
        return matches

    category_matcher = CategoryMatcher()
    category_matcher.clear()
    category_matcher.process_registers(
        parsers.slave.categories, parsers.master.categories
    )

    matches.category.globals.add_matches(category_matcher.pure_matches)
    matches.category.masterless.add_matches(
        category_matcher.masterless_matches)
    matches.category.slaveless.add_matches(category_matcher.slaveless_matches)

    if Registrar.DEBUG_CATS:
        if category_matcher.pure_matches:
            Registrar.register_message(
                "All Category matches on cat_name:\n%s" % (
                    '\n'.join(map(str, category_matcher.matches))))

    # using valid because the category tree can collapse multiple
    # master categories into single slave

    matches.category.valid += category_matcher.pure_matches

    if category_matcher.duplicate_matches:
        matches.category.duplicate['title'] = \
            category_matcher.duplicate_matches

        for match in category_matcher.duplicate_matches:
            master_taxo_sums = [cat.namesum for cat in match.m_objects]
            # If there is more than one master category in the match,
            # it is only valid if they have the same name
            if (
                len(master_taxo_sums) > 1
                and all(master_taxo_sums)
                and SeqUtils.check_equal(master_taxo_sums)
            ):
                if len(match.s_objects) == 1:
                    if len(match.m_objects) > 1:
                        deepest_m_object = sorted([
                            (m_object.depth, m_object)
                            for m_object in match.m_objects
                        ])[-1][1]
                        # Other matches are irrelevant if they have the same
                        # name
                        match = Match([deepest_m_object], match.s_objects)
                        # other_match = Match(other_m_objects)
                        # matches.category.slaveless.append(other_match)
                    matches.category.valid.append(match)
                    continue
                if len(match.s_objects) == 0:
                    matches.category.slaveless.append(match)
                    continue
            matches.category.invalid.append(match)
        if matches.category.invalid:
            exc = UserWarning(
                "categories couldn't be synchronized because of "
                "ambiguous names:\n%s"
                % '\n'.join(map(str, matches.category.invalid)))
            Registrar.register_error(exc)
            raise exc

    if not (matches.category.slaveless and matches.category.masterless):
        return matches

    # TODO: Now try and match the masterless / slaveless categories on title
    # instead of cat_name
    master_orphaned_categories = OrderedDict([
        (category.rowcount, category)
        for category in itertools.chain(*[
            match.m_objects for match in matches.category.slaveless])
    ])
    slave_orphaned_categories = OrderedDict([
        (category.title, category)
        for category in itertools.chain(*[
            match.s_objects for match in matches.category.masterless])
    ])

    title_matcher = CategoryTitleMatcher()
    title_matcher.clear()
    title_matcher.process_registers(
        slave_orphaned_categories, master_orphaned_categories
    )

    matches.category.globals.add_matches(title_matcher.pure_matches)
    matches.category.valid += title_matcher.pure_matches
    matches.category.masterless = MatchList(title_matcher.masterless_matches)
    matches.category.slaveless = MatchList(title_matcher.slaveless_matches)

    if matches.category.slaveless and matches.category.masterless:
        exc = UserWarning(
            "You may want to fix up the following "
            "categories before syncing:\n%s\n%s"
            %
            ('\n'.join(map(str, category_matcher.slaveless_matches)),
             '\n'.join(map(str, category_matcher.masterless_matches))))

        Registrar.register_error(exc)
        # raise exc

    return matches


# TODO: do_match_attributes ?
def do_match_attributes(parsers, matches, settings):
    if settings.do_attributes:
        raise NotImplementedError("Do Match Attributes not implemented")


def do_match_prod(parsers, matches, settings):
    """For every item in slave, find its counterpart in master."""

    Registrar.register_progress("Attempting matching")

    if not settings.do_sync:
        return matches

    product_matcher = ProductMatcher()
    product_matcher.process_registers(
        parsers.slave.products, parsers.master.products
    )
    # print product_matcher.__repr__()

    matches.globals.add_matches(product_matcher.pure_matches)
    matches.masterless.add_matches(product_matcher.masterless_matches)
    matches.deny_anomalous(
        'product_matcher.masterless_matches',
        product_matcher.masterless_matches
    )
    matches.slaveless.add_matches(product_matcher.slaveless_matches)
    matches.deny_anomalous(
        'product_matcher.slaveless_matches', product_matcher.slaveless_matches
    )

    try:
        matches.deny_anomalous(
            'product_matcher.duplicate_matches',
            product_matcher.duplicate_matches,
            True
        )
    except AssertionError as exc:
        exc = UserWarning(
            "products couldn't be synchronized because of ambiguous SKUs:%s"
            % '\n'.join(map(str, product_matcher.duplicate_matches)))
        Registrar.register_error(exc)
        raise exc


def do_match_var(parsers, matches, settings):
    # TODO: finish and test
    matches.variation = MatchNamespace(
        index_fn=ProductMatcher.product_index_fn
    )

    if not settings.do_variations:
        return

    variation_matcher = VariationMatcher()
    variation_matcher.process_registers(
        parsers.slave.variations, parsers.master.variations
    )

    if Registrar.DEBUG_VARS:
        Registrar.register_message("variation matcher:\n%s" %
                                   variation_matcher.__repr__())

    matches.variation.globals.add_matches(variation_matcher.pure_matches)
    matches.variation.masterless.add_matches(
        variation_matcher.masterless_matches)
    matches.variation.deny_anomalous(
        'variation_matcher.masterless_matches',
        variation_matcher.masterless_matches
    )
    matches.variation.slaveless.add_matches(
        variation_matcher.slaveless_matches)
    matches.variation.deny_anomalous(
        'variation_matcher.slaveless_matches',
        variation_matcher.slaveless_matches
    )
    if variation_matcher.duplicate_matches:
        matches.variation.duplicate['index'] = \
            variation_matcher.duplicate_matches


def do_merge_images(matches, parsers, updates, settings):
    if not getattr(updates, 'image', None):
        updates.image = UpdateNamespace()

    if not hasattr(matches, 'image'):
        return updates

    sync_handles = settings.sync_handles_img

    for match in matches.image.valid:
        m_object = match.m_object
        for s_object in match.s_objects:

            sync_update = settings.syncupdate_class_img(m_object, s_object)

            sync_update.update(sync_handles)

            if sync_update.m_updated:
                updates.image.master.append(sync_update)

            if not sync_update.important_static:
                updates.image.problematic.append(sync_update)
                continue

            if sync_update.s_updated:
                updates.image.slave.append(sync_update)

    for count, match in enumerate(matches.image.slaveless):
        m_object = match.m_object
        Registrar.register_message(
            "will create image %d: %s" % (
                count, m_object.identifier
            )
        )
        if not (m_object.attaches.products or m_object.attaches.categories):
            continue

        empty_s_object = parsers.slave.get_empty_attachment_instance()
        sync_update = settings.syncupdate_class_img(
            m_object, empty_s_object
        )
        sync_update.update(sync_handles)
        updates.image.slaveless.append(sync_update)

    for count, match in enumerate(matches.image.masterless):
        assert match.has_no_master
        s_object = match.s_object

        sync_update = settings.syncupdate_class_img(None, s_object)
        Registrar.register_message("will delete image: %d:\n%s" % (
            count, s_object.identifier))

        updates.image.masterless.append(sync_update)

    return updates


def get_update_cat(settings, m_object, s_object):
    sync_update = settings.syncupdate_class_cat(m_object, s_object)
    sync_update.update(settings.sync_handles_cat)

    if settings.do_images:
        sync_update.simplify_sync_warning_value_singular(
            'image', ['id', 'title', 'source_url'])

    return sync_update


def do_merge_categories(matches, parsers, updates, settings):
    if not hasattr(updates, 'category'):
        updates.category = UpdateNamespace()

    if not hasattr(matches, 'category'):
        return updates

    sync_handles = settings.sync_handles_cat

    for match in matches.category.valid:
        s_object = match.s_object
        for m_object in match.m_objects:

            sync_update = get_update_cat(settings, m_object, s_object)
            sync_update = settings.syncupdate_class_cat(m_object, s_object)

            sync_update.update(sync_handles)

            if settings.do_images:
                sync_update.simplify_sync_warning_value_singular(
                    'image', ['id', 'title', 'source_url'])

            if sync_update.m_updated:
                updates.category.master.append(sync_update)

            if not sync_update.important_static:
                updates.category.problematic(sync_update)
                continue

            if sync_update.s_updated:
                updates.category.slave.append(sync_update)

    if settings.auto_create_new:
        for count, match in enumerate(matches.category.slaveless):
            # not all masterless matches have a singular master object.
            # Only select the deepest one.
            if len(match.m_objects) > 1:
                m_object = sorted([
                    (m_object.depth, m_object) for m_object in match.m_objects
                ])[-1][1]
            else:
                m_object = match.m_object

            # TODO: if there is a pending change to change a slave category to
            # the same category name as one being created, then there will be a
            # conflict
            Registrar.register_message(
                "will create category %d: %s" % (
                    count, m_object.identifier
                )
            )
            empty_s_object = parsers.slave.get_empty_category_instance()
            sync_update = settings.syncupdate_class_cat(
                m_object, empty_s_object)
            sync_update.update(sync_handles)
            updates.category.slaveless.append(sync_update)

    return updates

# TODO: do_merge_attributes ?


def do_merge_attributes(matches, parsers, updates, settings):
    if settings.do_attributes:
        raise NotImplementedError("Do Merge Attributes not implemented")


def get_update_prod(settings, m_object, s_object):
    """Return a description of updates required to merge these products."""

    sync_update = settings.syncupdate_class_prod(m_object, s_object)
    sync_update.update(settings.sync_handles_prod)

    if settings.do_categories:
        sync_update.simplify_sync_warning_value_listed(
            'product_categories', ['term_id'])

    if settings.do_images:
        sync_update.simplify_sync_warning_value_listed(
            'attachment_objects', ['id', 'title', 'source_url', 'position']
        )

    if settings.do_attributes:
        sync_update.simplify_sync_warning_value_listed(
            'attributes', ['term_id'])

    return sync_update


def do_merge_prod(matches, parsers, updates, settings):
    """Return a description of updates required to merge `matches`."""

    if not settings.do_sync:
        return

    for count, match in enumerate(matches.globals):
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message("processing match %d:\n%s" % (
                count, match.tabulate()))
        m_object = match.m_object
        s_object = match.s_object

        # , "gcs %s is not variation but object is" % repr(gcs)
        assert not m_object.is_variation
        # , "gcs %s is not variation but object is" % repr(gcs)
        assert not s_object.is_variation

        if m_object.get('post_status') == 'trash':
            matches.masterless.append(match)
            continue

        sync_update = get_update_prod(settings, m_object, s_object)

        # Assumes that GDrive is read only, doesn't care about master
        #   updates

        if Registrar.DEBUG_VARS:
            Registrar.register_message("prod update %d:\n%s" % (
                count, sync_update.tabulate()))

        if sync_update.m_updated:
            updates.master.append(sync_update)

        if not sync_update.s_updated:
            continue

        if Registrar.DEBUG_UPDATE:
            Registrar.register_message("update %d:\n%s" % (
                count, sync_update.tabulate()
            ))

        if sync_update.s_updated and sync_update.s_deltas:
            updates.delta_slave.append(sync_update)

        if not sync_update.important_static:
            updates.problematic.append(sync_update)
            continue

        if sync_update.s_updated:
            updates.slave.append(sync_update)

    for count, match in enumerate(matches.slaveless):
        m_object = match.m_object

        if m_object.get('post_status') == 'trash':
            continue

        Registrar.register_message(
            "will create product %d: %s" % (
                count, m_object.identifier
            )
        )
        sync_update = get_update_prod(settings, m_object, None)
        updates.slaveless.append(sync_update)

    for count, match in enumerate(matches.masterless):
        s_object = match.s_object

        Registrar.register_message("will delete product: %d:\n%s" % (
            count, s_object.identifier))

        sync_update = get_update_prod(settings, None, s_object)

        updates.masterless.append(sync_update)


def get_update_var(settings, m_object, s_object):
    sync_update = settings.syncupdate_class_var(m_object, s_object)
    sync_update.update(settings.sync_handles_var)

    if settings.do_images:
        sync_update.simplify_sync_warning_value_singular(
            'image', ['id', 'title', 'source_url'])

    if settings.do_attributes:
        sync_update.simplify_sync_warning_value_listed(
            'attributes', ['term_id'])

    return sync_update


def do_merge_var(matches, parsers, updates, settings):
    if not (settings.do_variations and settings.do_sync):
        return

    if not hasattr(updates, 'variation'):
        updates.variation = UpdateNamespace()

    if matches.variation.duplicate:
        exc = UserWarning(
            "variations couldn't be synchronized because of ambiguous SKUs:%s"
            % '\n'.join(map(str, matches.variation.duplicate)))
        Registrar.register_error(exc)
        raise exc

    for count, match in enumerate(matches.variation.globals):
        if Registrar.DEBUG_VARS:
            Registrar.register_message("processing match %d:\n%s" % (
                count, match.tabulate()))
        m_object = match.m_object
        s_object = match.s_object

        if m_object.get('post_status') == 'trash':
            matches.variation.masterless.append(match)
            continue

        sync_update = get_update_var(settings, m_object, s_object)

        # Assumes that GDrive is read only, doesn't care about master
        #   updates

        if Registrar.DEBUG_VARS:
            Registrar.register_message("var update %d:\n%s" % (
                count, sync_update.tabulate()))

        if sync_update.s_updated and sync_update.s_deltas:
            updates.variation.delta_slave.append(sync_update)

        if sync_update.m_updated:
            updates.variation.master.append(sync_update)

        if not sync_update.important_static:
            updates.variation.problematic.append(sync_update)
            continue

        if sync_update.s_updated:
            updates.variation.slave.append(sync_update)

    for count, match in enumerate(matches.variation.slaveless):
        m_object = match.m_object

        if m_object.get('post_status') == 'trash':
            continue

        sync_update = get_update_var(settings, m_object, None)

        Registrar.register_message("Will create variation %d:\n%s" % (
            count, m_object.identifier))

        updates.variation.slaveless.append(sync_update)
        # TODO: figure out which attribute terms to add to parent?

    for count, match in enumerate(matches.variation.masterless):
        s_object = match.s_object

        Registrar.register_message("will delete variation: %d:\n%s" % (
            count, s_object.identifier))

        sync_update = get_update_var(settings, None, s_object)

        updates.variation.masterless.append(sync_update)
        # TODO: figure out which attribute terms to delete from parent?


def do_report_images(reporters, matches, updates, parsers, settings):
    if not settings.get('do_report'):
        return reporters

    Registrar.register_progress("Write Images Report")

    do_img_sync_group(reporters.img, matches, updates, parsers, settings)

    if settings.get('report_matching'):
        do_matches_group(
            reporters.img, matches.image, updates, parsers, settings)

    if reporters.img:
        reporters.img.write_document_to_file('img', settings.rep_img_path)

    return reporters


def do_report_categories(reporters, matches, updates, parsers, settings):
    if not settings.get('do_report'):
        return reporters

    Registrar.register_progress("Write Categories Report")

    do_cat_sync_gruop(reporters.cat, matches, updates, parsers, settings)

    if reporters.cat:
        reporters.cat.write_document_to_file('cat', settings.rep_cat_path)

    return reporters

# TODO: do_report_attributes ?


def do_report_attributes(reporters, matches, updates, parsers, settings):
    if settings.do_attributes:
        raise NotImplementedError("Do Report Attributes not implemented")


def do_report(reporters, matches, updates, parsers, settings):
    """ Write report of changes to be made. """

    if not settings.get('do_report'):
        return reporters

    Registrar.register_progress("Write Report")

    do_main_summary_group(
        reporters.main, matches, updates, parsers, settings
    )
    do_delta_group(
        reporters.main, matches, updates, parsers, settings
    )
    do_sync_group(
        reporters.main, matches, updates, parsers, settings
    )
    do_var_sync_group(
        reporters.main, matches, updates, parsers, settings
    )

    if reporters.main:
        reporters.main.write_document_to_file('main', settings.rep_main_path)

    if settings.get('report_matching'):
        Registrar.register_progress("Write Matching Report")

        do_matches_summary_group(
            reporters.match, matches, updates, parsers, settings
        )
        do_matches_group(
            reporters.match, matches, updates, parsers, settings
        )
        if settings.do_variations:
            do_variation_matches_group(
                reporters.match, matches, updates, parsers, settings
            )
        if settings.do_categories:
            do_category_matches_group(
                reporters.match, matches, updates, parsers, settings
            )

        if reporters.match:
            reporters.match.write_document_to_file(
                'match', settings.rep_match_path)

    return reporters


def do_report_post(reporters, results, settings):
    """ Reports results from performing updates."""
    # raise NotImplementedError()
    if settings.get('do_report'):
        Registrar.register_progress("Write Post Report")

        do_post_summary_group(reporters.post, results, settings)
        do_failures_group(reporters.post, results, settings)
        do_successes_group(reporters.post, results, settings)
        if reporters.post:
            reporters.post.write_document_to_file(
                'post', settings.rep_post_path)


def handle_failed_update(update, results, exc, settings, source=None):
    """Handle a failed update."""
    fail = (update, exc)
    if source == settings.master_name:
        pkey = update.master_id
        results.fails_master.append(fail)
    elif source == settings.slave_name:
        pkey = update.slave_id
        results.fails_slave.append(fail)
    else:
        pkey = ''
    Registrar.register_error(
        "ERROR UPDATING %s (%s): %s\n%s\n%s" % (
            source or '',
            pkey,
            repr(exc),
            update.tabulate(),
            traceback.format_exc()
        )
    )

    if Registrar.DEBUG_TRACE:
        pudb.set_trace()


def usr_prompt_continue(settings):
    # TODO: this is completely broken. just read from stdin
    if not settings.ask_before_update:
        return
    try:
        raw_in = input("\n".join([
            "Please read reports and then make your selection",
            " - press Enter to continue and perform updates",
            " - press s to skip updates",
            " - press c to cancel",
            "..."
        ]))
    except SyntaxError:
        raw_in = ""
    if raw_in == 's':
        return 's'
    if raw_in == 'c':
        raise SystemExit


def upload_new_items_slave(
    parsers, results, settings, client, new_updates, _type='product'
):

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(new_updates), items_plural='new %s(s)' % _type
        )

    if not (new_updates and settings.update_slave):
        return

    update_count = 0

    type_get_update_fns = {
        'category': get_update_cat,
        'product': get_update_prod,
        'variation': get_update_var
    }

    type_analyse_api_obj_fns = {
        'image': 'analyse_api_image_raw',
        'category': 'process_api_category_raw',
        'product': 'analyse_api_obj',
        # TODO: fill in this
        'variation': 'process_api_variation_raw'
    }

    while new_updates:

        sync_update = new_updates.pop(0)

        if _type == "category":
            # make sure parent updates are done before children
            new_object_gen = sync_update.old_m_object_gen

            if new_object_gen.parent:
                remaining_m_objects = set([
                    sync_update.old_m_object_gen for update_ in new_updates
                ])
                parent = new_object_gen.parent
                if not parent.is_root and parent in remaining_m_objects:
                    new_updates.append(sync_update)
                    continue

        if _type in type_get_update_fns:
            # have to refresh sync_update to get parent wpid since parente
            # wpid is populated in do_updates_categories_master
            sync_update = type_get_update_fns.get(_type)(
                settings,
                sync_update.old_m_object,
                sync_update.old_s_object
            )

        core_data = sync_update.get_slave_updates()

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "new %s (core format) %s" % (
                    _type,
                    core_data
                )
            )

        update_count += 1
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(update_count)

        create_item_kwargs = {}
        process_item_kwargs = {}
        if _type == 'variation':
            parent_id = sync_update.old_m_object.parent['ID']
            create_item_kwargs['parent_pkey'] = parent_id
            process_item_kwargs['parent_id'] = parent_id

        try:
            response = client.create_item_core(core_data, **create_item_kwargs)
            response_api_data = response.json()
        except BaseException as exc:
            handle_failed_update(
                sync_update, results, exc, settings, settings.slave_name
            )
            continue
        if client.page_nesting:
            response_api_data = response_api_data[client.endpoint_singular]

        response_gen_object = getattr(
            parsers.slave, type_analyse_api_obj_fns[_type]
        )(
            response_api_data, **process_item_kwargs
        )

        sync_update.set_new_s_object_gen(response_gen_object)
        sync_update.old_m_object_gen.update(response_gen_object)
        results.successes.append(sync_update)


# TODO: collapse upload_new functions
def upload_new_images_slave(parsers, results, settings, client, new_updates):

    # TODO: fix constantly re-uploading images

    upload_new_items_slave(
        parsers, results, settings, client, new_updates, _type="image"
    )


# TODO: collapse upload_changes functions
def upload_image_changes_slave(
    parsers, results, settings, client, change_updates
):
    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(change_updates),
            items_plural='%s updates' % client.endpoint_singular
        )

    if not settings.update_slave:
        return

    for count, sync_update in enumerate(change_updates):
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(count)

        if not sync_update.s_updated:
            continue

        try:
            pkey = sync_update.slave_id
            changes = sync_update.get_slave_updates()
            response_raw = client.upload_changes_core(pkey, changes)
            response_api_data = response_raw.json()
            if client.page_nesting:
                response_api_data = response_api_data[client.endpoint_singular]
        except Exception as exc:
            handle_failed_update(
                sync_update, results, exc, settings, settings.slave_name
            )
            continue

        if response_api_data['id'] != pkey:
            # if pkey has changed since update, i.e. a new item was uploaded
            response_gen_object = parsers.slave.analyse_api_image_raw(
                response_api_data)
        else:
            response_core_data = \
                settings.coldata_class_img.translate_data_from(
                    response_api_data, settings.coldata_img_target
                )
            response_gen_data = settings.coldata_class_img.translate_data_to(
                response_core_data, settings.coldata_gen_target_write
            )
            sync_update.old_s_object_gen.update(response_gen_data)
            response_gen_object = sync_update.old_s_object_gen

        sync_update.set_new_s_object_gen(response_gen_object)
        sync_update.old_m_object_gen.update(response_gen_object)

        results.successes.append(sync_update)


def do_updates_images_master(updates, parsers, results, settings):
    for update in updates.image.master:
        old_master_id = update.master_id
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "performing update < %5s | %5s > = \n%100s, %100s " %
                (update.master_id, update.slave_id,
                 str(update.old_m_object), str(update.old_s_object)))
        if old_master_id not in parsers.master.attachments:
            exc = UserWarning(
                "couldn't fine pkey %s in parsers.master.attachments" %
                update.master_id)
            Registrar.register_error(exc)
            continue
        parsers.master.attachments[old_master_id].update(
            update.get_master_updates_native()
        )


def delete_images_slave(parsers, results, settings, client, delete_updates):
    raise NotImplementedError()


def do_updates_images_slave(updates, parsers, results, settings):
    """Perform a list of updates on attachments."""

    results.image = ResultsNamespace()
    results.image.new = ResultsNamespace()

    sync_client_class = settings.slave_img_sync_client_class
    sync_client_args = settings.slave_img_sync_client_args

    try:
        endpoint_singular = sync_client_class.endpoint_singular
        assert isinstance(endpoint_singular, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_singular = "image"

    try:
        endpoint_plural = sync_client_class.endpoint_plural
        assert isinstance(endpoint_plural, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_plural = "%s" % endpoint_singular

    # updates in which an item is modified
    change_updates = updates.image.slave
    if settings.do_problematic:
        change_updates += updates.image.problematic
    # updates in which a new item is created
    new_updates = []
    if settings.auto_create_new:
        new_updates += updates.image.slaveless
    else:
        for update in updates.image.slaveless:
            new_item_api = update.get_slave_updates_native()
            exc = UserWarning("{0} needs to be created: {1}".format(
                endpoint_singular, new_item_api
            ))
            Registrar.register_warning(exc)

    delete_updates = []
    if settings.auto_delete_old:
        delete_updates += updates.image.masterless
    else:
        for update in updates.image.masterless:
            deleted_item_api = update.get_slave_updates_native()
            exc = UserWarning("{0} needs to be deleted: {1}".format(
                endpoint_singular, deleted_item_api
            ))
            Registrar.register_warning(exc)

    Registrar.register_progress(
        "Changing {1}, creating {2} and deleting {3} {0}".format(
            endpoint_plural, len(change_updates), len(new_updates),
            len(delete_updates)
        )
    )

    if not (new_updates or change_updates or delete_updates):
        return

    if settings['ask_before_update']:
        if usr_prompt_continue(settings) == 's':
            return

    with sync_client_class(**sync_client_args) as client:
        if new_updates:
            upload_new_images_slave(
                parsers, results.image.new, settings, client, new_updates
            )

        if change_updates:
            upload_image_changes_slave(
                parsers, results.image, settings, client, change_updates
            )

        if delete_updates:
            delete_images_slave(
                parsers, results, settings, client, delete_updates
            )


def upload_new_categories_slave(
    parsers, results, settings, client, new_updates
):
    """
    Create new categories in client in an order which creates parents first.
    """
    upload_new_items_slave(
        parsers, results, settings, client, new_updates, _type="category"
    )


def upload_category_changes_slave(
    parsers, results, settings, client, change_updates
):
    """
    Upload a list of category changes
    """

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(change_updates),
            items_plural='%s updates' % client.endpoint_singular
        )

    if not settings.update_slave:
        return

    for count, sync_update in enumerate(change_updates):
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(count)

        if not sync_update.s_updated:
            continue

        try:
            pkey = sync_update.slave_id
            changes = sync_update.get_slave_updates_native()
            response_raw = client.upload_changes(pkey, changes)
            response_api_data = response_raw.json()
        except Exception as exc:
            handle_failed_update(
                sync_update, results, exc, settings, settings.slave_name
            )
            continue

        response_core_data = settings.coldata_class_cat.translate_data_from(
            response_api_data, settings.coldata_cat_target
        )
        response_gen_data = settings.coldata_class_cat.translate_data_to(
            response_core_data, settings.coldata_gen_target_write
        )

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "%s being updated with parser data: %s" % (
                    client.endpoint_singular,
                    pformat(response_gen_data)
                )
            )

        sync_update.old_s_object_gen.update(response_gen_data)
        sync_update.set_new_s_object_gen(sync_update.old_s_object_gen)
        sync_update.old_m_object_gen.update(response_gen_data)

        results.successes.append(sync_update)


def do_updates_categories_master(updates, parsers, results, settings):
    for update in updates.category.master:
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "performing update < %5s | %5s > = \n%100s, %100s " %
                (
                    update.master_id, update.slave_id,
                    str(update.old_m_object), str(update.old_s_object))
                )
        if update.master_id not in parsers.master.categories:
            exc = UserWarning(
                "couldn't fine pkey %s in parsers.master.categories" %
                update.master_id)
            Registrar.register_error(exc)
            continue
        parsers.master.categories[update.master_id].update(
            update.get_master_updates_native()
        )


def delete_categories_slave(
    parsers, results, settings, client, delete_updates
):
    raise NotImplementedError()


def do_updates_categories_slave(updates, parsers, results, settings):
    """Perform a list of updates on categories."""
    if not hasattr(updates, 'category'):
        return

    results.category = ResultsNamespace()
    results.category.new = ResultsNamespace()

    sync_client_class = settings.slave_cat_sync_client_class
    sync_client_args = settings.slave_cat_sync_client_args

    try:
        endpoint_singular = sync_client_class.endpoint_singular
        assert isinstance(endpoint_singular, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_singular = "product"

    try:
        endpoint_plural = sync_client_class.endpoint_plural
        assert isinstance(endpoint_plural, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_plural = "%s" % endpoint_singular

    change_updates = updates.category.slave
    if settings.do_problematic:
        change_updates += updates.category.problematic
    # updates in which a new item is created
    new_updates = []
    if settings.auto_create_new:
        new_updates += updates.category.slaveless
    else:
        for update in updates.category.slaveless:
            new_item_api = update.get_slave_updates_native()
            exc = UserWarning("{0} needs to be created: {1}".format(
                endpoint_singular, new_item_api
            ))
            Registrar.register_warning(exc)

    delete_updates = []
    if settings.auto_delete_old:
        delete_updates += updates.category.masterless
    else:
        for update in updates.category.masterless:
            deleted_item_api = update.get_slave_updates_native()
            exc = UserWarning("{0} needs to be deleted: {1}".format(
                endpoint_singular, deleted_item_api
            ))
            Registrar.register_warning(exc)

    Registrar.register_progress(
        "Changing {1}, creating {2} and deleting {3} {0}".format(
            endpoint_plural, len(change_updates), len(new_updates),
            len(delete_updates)
        )
    )

    if not (new_updates or change_updates or delete_updates):
        return

    if settings['ask_before_update']:
        if usr_prompt_continue(settings) == 's':
            return

    with sync_client_class(**sync_client_args) as client:
        if new_updates:
            upload_new_categories_slave(
                parsers, results.category.new, settings, client, new_updates
            )

        if change_updates:
            upload_category_changes_slave(
                parsers, results.category, settings, client, change_updates
            )

        if delete_updates:
            delete_categories_slave(
                parsers, results, settings, client, delete_updates
            )


# TODO: do_updates_attributes_master ?
def do_updates_attributes_master(updates, parsers, results, settings):
    if settings.do_attributes:
        raise NotImplementedError(
            "Do Updates Attributes Master not implemented")


# TODO: do_updates_attributes_slave ?
def do_updates_attributes_slave(updates, parsers, results, settings):
    if settings.do_attributes:
        raise NotImplementedError(
            "Do Updates Attributes Slave not implemented")


def upload_new_products(parsers, results, settings, client, new_updates):
    """
    Create new products in client in an order which creates parents first.
    """

    upload_new_items_slave(
        parsers, results, settings, client, new_updates, _type="product"
    )


def upload_product_changes(parsers, results, settings, client, change_updates):

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(change_updates),
            items_plural='%s updates' % client.endpoint_singular
        )

    if not settings.update_slave:
        return

    for count, sync_update in enumerate(change_updates):
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(count)

        if not sync_update.s_updated:
            continue

        try:
            pkey = sync_update.slave_id
            changes = sync_update.get_slave_updates_native()
            response_raw = client.upload_changes(pkey, changes)
            response_api_data = response_raw.json()
        except Exception as exc:
            handle_failed_update(
                sync_update, results, exc, settings, settings.slave_name
            )
            continue

        response_core_data = settings.coldata_class.translate_data_from(
            response_api_data, settings.coldata_cat_target
        )
        response_gen_data = settings.coldata_class.translate_data_to(
            response_core_data, settings.coldata_gen_target_write
        )

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "%s being updated with parser data: %s" % (
                    client.endpoint_singular,
                    pformat(response_gen_data)
                )
            )

        sync_update.old_s_object_gen.update(response_gen_data)
        sync_update.set_new_s_object_gen(sync_update.old_s_object_gen)
        sync_update.old_m_object_gen.update(response_gen_data)

        results.successes.append(sync_update)


def delete_products_slave(parsers, results, settings, client, delete_updates):
    raise NotImplementedError()


def do_updates_prod_slave(updates, parsers, results, settings):
    """
    Update products in slave.
    """
    # updates in which an item is modified

    results.new = ResultsNamespace()
    sync_client_class = settings.slave_upload_client_class
    sync_client_args = settings.slave_upload_client_args

    try:
        endpoint_singular = sync_client_class.endpoint_singular
        assert isinstance(endpoint_singular, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_singular = "product"

    try:
        endpoint_plural = sync_client_class.endpoint_plural
        assert isinstance(endpoint_plural, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_plural = "%s" % endpoint_singular

    change_updates = updates.slave
    if settings.do_problematic:
        change_updates += updates.problematic
    # updates in which a new item is created
    new_updates = []
    if settings.auto_create_new:
        new_updates += updates.slaveless
    else:
        for update in updates.slaveless:
            new_item_api = update.get_slave_updates_native()
            exc = UserWarning("{0} needs to be created: {1}".format(
                endpoint_singular, new_item_api
            ))
            Registrar.register_warning(exc)

    delete_updates = []
    if settings.auto_delete_old:
        delete_updates += updates.masterless
    else:
        for update in updates.masterless:
            deleted_item_api = update.get_slave_updates_native()
            exc = UserWarning("{0} needs to be deleted: {1}".format(
                endpoint_singular, deleted_item_api
            ))
            Registrar.register_warning(exc)

    Registrar.register_progress(
        "Changing {1}, creating {2} and deleting {3} {0}".format(
            endpoint_plural, len(change_updates), len(new_updates),
            len(delete_updates)
        )
    )

    if not (new_updates or change_updates or delete_updates):
        return

    if settings['ask_before_update']:
        if usr_prompt_continue(settings) == 's':
            return

    with sync_client_class(**sync_client_args) as client:
        if new_updates:
            upload_new_products(
                parsers, results, settings, client, new_updates
            )
        if change_updates:
            upload_product_changes(
                parsers, results, settings, client, change_updates
            )

        if delete_updates:
            delete_products_slave(
                parsers, results, settings, client, delete_updates
            )


def do_updates_prod_master(updates, parsers, settings, results):
    for update in updates.master:
        old_master_id = update.master_id
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "performing update < %5s | %5s > = \n%100s, %100s " %
                (update.master_id, update.slave_id,
                 str(update.old_m_object), str(update.old_s_object)))
        if old_master_id not in parsers.master.items:
            exc = UserWarning(
                "couldn't fine pkey %s in parsers.master.items" %
                update.master_id)
            Registrar.register_error(exc)
            continue
        parsers.master.items[old_master_id].update(
            update.get_master_updates_native()
        )


def do_updates_var_master(updates, parsers, results, settings):
    for update in updates.variation.master:
        old_master_id = update.master_id
        if Registrar.DEBUG_UPDATE:
            Registrar.register_message(
                "performing update < %5s | %5s > = \n%100s, %100s " %
                (update.master_id, update.slave_id,
                 str(update.old_m_object), str(update.old_s_object)))
        if old_master_id not in parsers.master.variations:
            exc = UserWarning(
                "couldn't fine pkey %s in parsers.master.attachments" %
                update.master_id)
            Registrar.register_error(exc)
            continue
        parsers.master.variations[old_master_id].update(
            update.get_master_updates_native()
        )


def upload_new_variations_slave(
    parsers, results, settings, client, new_updates
):
    upload_new_items_slave(
        parsers, results, settings, client, new_updates, _type="variation"
    )


def upload_variation_changes_slave(
    parsers, results, settings, client, change_updates
):
    """
    Upload a list of variation changes
    """

    try:
        endpoint_singular = client.endpoint_singular
        assert isinstance(endpoint_singular, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_singular = "variation"

    try:
        endpoint_plural = client.endpoint_plural
        assert isinstance(endpoint_plural, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_plural = "%ss" % endpoint_singular

    if Registrar.DEBUG_PROGRESS:
        update_progress_counter = ProgressCounter(
            len(change_updates),
            items_plural='%s updates' % endpoint_singular
        )

    if not settings.update_slave:
        return

    for count, sync_update in enumerate(change_updates):
        if Registrar.DEBUG_PROGRESS:
            update_progress_counter.maybe_print_update(count)

        if not sync_update.s_updated:
            continue

        try:
            pkey = sync_update.slave_id
            parent_pkey = sync_update.new_s_object['parent_id']
            changes = sync_update.get_slave_updates_native()
            response_raw = client.upload_changes(
                pkey, changes, parent_pkey=parent_pkey)
            response_api_data = response_raw.json()
        except Exception as exc:
            handle_failed_update(
                sync_update, results, exc, settings, settings.slave_name
            )
            continue

        response_core_data = settings.coldata_class_cat.translate_data_from(
            response_api_data, settings.coldata_cat_target
        )
        response_gen_data = settings.coldata_class_cat.translate_data_to(
            response_core_data, settings.coldata_gen_target_write
        )

        if Registrar.DEBUG_API:
            Registrar.register_message(
                "%s being updated with parser data: %s" % (
                    endpoint_singular,
                    pformat(response_gen_data)
                )
            )

        sync_update.old_s_object_gen.update(response_gen_data)
        sync_update.set_new_s_object_gen(sync_update.old_s_object_gen)
        sync_update.old_m_object_gen.update(response_gen_data)

        results.successes.append(sync_update)


def delete_variations_slave(
    parsers, results, settings, client, delete_updates
):
    raise NotImplementedError()


def do_updates_var_slave(updates, parsers, results, settings):
    results.variation = ResultsNamespace()
    results.variation.new = ResultsNamespace()

    sync_client_class = settings.slave_var_sync_client_class
    sync_client_args = settings.slave_var_sync_client_args

    try:
        endpoint_singular = sync_client_class.endpoint_singular
        assert isinstance(endpoint_singular, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_singular = "variation"

    try:
        endpoint_plural = sync_client_class.endpoint_plural
        assert isinstance(endpoint_plural, string_types)
    except (AssertionError, AttributeError, UserWarning):
        endpoint_plural = "%ss" % endpoint_singular

    change_updates = updates.variation.slave
    if settings.do_problematic:
        change_updates += updates.variation.problematic

    new_updates = []
    if settings.auto_create_new:
        new_updates += updates.variation.slaveless
    else:
        for update in updates.variation.slaveless:
            new_item_api = update.get_slave_updates_native()
            exc = UserWarning("{0} needs to be created: {1}".format(
                endpoint_singular, new_item_api
            ))
            Registrar.register_warning(exc)

    delete_updates = []
    if settings.auto_delete_old:
        delete_updates += updates.variation.masterless
    else:
        for update in updates.variation.masterless:
            deleted_item_api = update.get_slave_updates_native()
            exc = UserWarning("{0} needs to be deleted: {1}".format(
                endpoint_singular, deleted_item_api
            ))
            Registrar.register_warning(exc)

    Registrar.register_progress(
        "Changing {1}, creating {2} and deleting {3} {0}".format(
            endpoint_plural, len(change_updates), len(new_updates),
            len(delete_updates)
        )
    )

    if not (new_updates or change_updates or delete_updates):
        return

    if settings['ask_before_update']:
        if usr_prompt_continue(settings) == 's':
            return

    with sync_client_class(**sync_client_args) as client:
        if new_updates:
            upload_new_variations_slave(
                parsers, results.variation.new, settings, client, new_updates
            )

        if change_updates:
            upload_variation_changes_slave(
                parsers, results.variation, settings, client, change_updates
            )

        if delete_updates:
            delete_variations_slave(
                parsers, results, settings, client, delete_updates
            )


def main(override_args=None, settings=None):
    """Main function for generator."""
    if not settings:
        settings = SettingsNamespaceProd()
    settings.init_settings(override_args)

    settings.init_dirs()

    ########################################
    # Create Product Parser object
    ########################################

    parsers = ParserNamespace()
    populate_master_parsers(parsers, settings)

    check_warnings(settings)

    if settings.schema_is_woo and settings.do_images:
        process_images(settings, parsers)

    if parsers.master.objects and settings.do_export_master:
        export_master_parser(settings, parsers)

    if settings.master_and_quit:
        sys.exit(ExitStatus.success)

    populate_slave_parsers(parsers, settings)

    if parsers.slave.objects:
        cache_api_data(settings, parsers)

    matches = MatchNamespace(
        index_fn=ProductMatcher.product_index_fn
    )
    updates = UpdateNamespace()
    reporters = ReporterNamespace()
    results = ResultsNamespace()

    if settings.do_images:
        do_match_images(parsers, matches, settings)
        do_merge_images(matches, parsers, updates, settings)
        do_report_images(
            reporters, matches, updates, parsers, settings
        )
        Registrar.register_message(
            "pre-sync summary: \n%s" % reporters.img.get_summary_text()
        )
        check_warnings(settings)
        if not settings.report_and_quit:
            do_updates_images_master(updates, parsers, results, settings)
            try:
                do_updates_images_slave(updates, parsers, results, settings)
            except (SystemExit, KeyboardInterrupt) as exc:
                Registrar.register_error(exc)
                return reporters, results

    if settings.do_categories:

        do_match_categories(parsers, matches, settings)
        do_merge_categories(matches, parsers, updates, settings)
        do_report_categories(
            reporters, matches, updates, parsers, settings
        )
        Registrar.register_message(
            "pre-sync summary: \n%s" % reporters.cat.get_summary_text()
        )
        check_warnings(settings)
        if not settings.report_and_quit:
            do_updates_categories_master(updates, parsers, results, settings)
            if settings.report_matched_categories:
                export_categories(
                    settings, parsers.master, settings.rep_matched_cat_path,
                    'wc-csv'
                )
                reporters.cat.add_csv_file(
                    'matched_cat', settings.rep_matched_cat_path)
            try:
                do_updates_categories_slave(
                    updates, parsers, results, settings)
            except (SystemExit, KeyboardInterrupt) as exc:
                Registrar.register_error(exc)
                return reporters, results

    if settings.do_attributes:
        Registrar.register_error(NotImplementedError(
            "Functions past this point have not been completed"))
        return reporters, results

        do_match_attributes(parsers, matches, settings)
        do_merge_attributes(matches, parsers, updates, settings)
        do_report_attributes(
            reporters, matches, updates, parsers, settings
        )
        Registrar.register_message(
            "pre-sync summary: \n%s" % reporters.attr.get_summary_text()
        )
        check_warnings(settings)
        if not settings.report_and_quit:
            do_updates_attributes_master(updates, parsers, results, settings)
            try:
                do_updates_attributes_slave(
                    updates, parsers, results, settings)
            except (SystemExit, KeyboardInterrupt) as exc:
                Registrar.register_error(exc)
                return reporters, results

    # product variation IDs must be known before prod sync but any non-existent
    # variable products must be created before their variations can be created.
    if settings.do_variations:
        do_match_var(parsers, matches, settings)
        do_merge_var(matches, parsers, updates, settings)
        # do_report_variations(
        #     reporters, matches, updates, parsers, settings
        # )
        Registrar.register_message(
            "pre-sync summary: \n%s" % reporters.var.get_summary_text()
        )
        check_warnings(settings)
        do_updates_var_master(updates, parsers, settings, results)

    do_match_prod(parsers, matches, settings)
    do_merge_prod(matches, parsers, updates, settings)

    # check_warnings(settings)
    do_report(reporters, matches, updates, parsers, settings)

    if settings.report_and_quit:
        sys.exit(ExitStatus.success)

    check_warnings(settings)

    Registrar.register_message(
        "pre-sync summary: \n%s" % reporters.main.get_summary_text()
    )

    try:
        do_updates_prod_slave(updates, parsers, results, settings)
        do_updates_prod_master(updates, parsers, results, settings)
    except (SystemExit, KeyboardInterrupt) as exc:
        Registrar.register_error(exc)
        return reporters, results

    if settings.do_variations:
        if not settings.report_and_quit:
            try:
                do_updates_var_slave(updates, parsers, results, settings)
            except (SystemExit, KeyboardInterrupt) as exc:
                Registrar.register_error(exc)
                return reporters, results

    do_report_post(reporters, results, settings)

    Registrar.register_message(
        "post-sync summary: \n%s" % reporters.post.get_summary_text()
    )

    #########################################
    # Display reports
    #########################################

    Registrar.register_progress("Displaying reports")

    if settings.do_report:
        if settings['rep_web_path']:
            shutil.copyfile(settings.rep_main_path, settings['rep_web_path'])
            if settings['web_browser']:
                os.environ['BROWSER'] = settings['web_browser']
                # print "set browser environ to %s" % repr(web_browser)
            # print "moved file from %s to %s" % (settings.rep_main_path,
            # repWeb_path)

            webbrowser.open(settings['rep_web_link'])
    else:
        print("open this link to view report %s" % settings['rep_web_link'])


def catch_main(override_args=None):
    """Run main within a try statement and attempt to analyse failure."""
    file_path = __file__
    cur_dir = os.getcwd() + '/'
    if file_path.startswith(cur_dir):
        file_path = file_path[len(cur_dir):]
    override_args_repr = ''
    if override_args is not None:
        override_args_repr = ' '.join(override_args)

    full_run_str = "%s %s %s" % (
        str(sys.executable), str(file_path), override_args_repr)

    settings = SettingsNamespaceProd()

    status = 0
    try:
        main(settings=settings, override_args=override_args)
    except SystemExit:
        status = ExitStatus.failure
    except KeyboardInterrupt:
        pass
    except BaseException as exc:
        status = 1
        if isinstance(exc, UserWarning):
            status = 65
        elif isinstance(exc, IOError):
            status = 74
            print("cwd: %s" % os.getcwd())
        elif exc.__class__ in [
            "ReadTimeout", "ConnectionError", "ConnectTimeout",
            "ServerNotFoundError"
        ]:
            status = 69  # service unavailable

        if status:
            Registrar.register_error(traceback.format_exc())
            Registrar.raise_exception(exc)

    with io.open(settings.log_path, 'w+', encoding='utf8') as log_file:
        for source, messages in Registrar.get_message_items(1).items():
            print(source)
            log_file.writelines([SanitationUtils.coerce_unicode(source)])
            log_file.writelines([
                SanitationUtils.coerce_unicode(message) for message in messages
            ])
            for message in messages:
                pprint(message, indent=4, width=80, depth=2)

    #########################################
    # zip reports
    #########################################

    files_to_zip = [
        settings.rep_fail_master_csv_path, settings.rep_fail_slave_csv_path,
        settings.rep_main_path
    ]

    with zipfile.ZipFile(settings.zip_path, 'w') as zip_file:
        for file_to_zip in files_to_zip:
            try:
                os.stat(file_to_zip)
                zip_file.write(file_to_zip)
            except BaseException:
                pass
        Registrar.register_message('wrote file %s' % zip_file.filename)

    # print "\nexiting with status %s \n" % status
    if status:
        print("re-run with: \n%s" % full_run_str)
    else:
        Registrar.register_message("re-run with:\n%s" % full_run_str)

    sys.exit(status)


if __name__ == '__main__':
    catch_main()
